{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê²°ì¬ ëŒ“ê¸€ ê²€ì¦ ë° ì¶”ì¶œ\n",
    "\n",
    "## ëª©ì \n",
    "1. **1ë‹¨ê³„**: DBì— ìˆëŠ” 100ê°œ ëŒ“ê¸€ ê¸°ì¤€ìœ¼ë¡œ HTML íŒŒì‹± ê²€ì¦\n",
    "2. **2ë‹¨ê³„**: ì „ì²´ HTML ìŠ¤ìº” ë° ëˆ„ë½ ëŒ“ê¸€ ì¶”ì¶œ (1ë‹¨ê³„ 100% ì¼ì¹˜ ì‹œ)\n",
    "3. **3ë‹¨ê³„**: DB INSERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì…€ 1: ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)\n",
    "# !pip install beautifulsoup4 lxml mysql-connector-python pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ê²½ê³  ë¬´ì‹œ\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì • ì™„ë£Œ\n",
      "HTML í´ë” ìˆ˜: 4ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ========== ì„¤ì • ==========\n",
    "\n",
    "# HTML íŒŒì¼ í´ë” ê²½ë¡œ (4ê°œ)\n",
    "HTML_FOLDERS = [\n",
    "    r\"C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\",\n",
    "    r\"C:\\Users\\LEEJUHWAN\\Downloads\\2011-01-01~2015-12-31\\html\\ê²°ì¬\",\n",
    "    r\"C:\\Users\\LEEJUHWAN\\Downloads\\2016-01-01~2020-12-31\\html\\ê²°ì¬\",\n",
    "    r\"C:\\Users\\LEEJUHWAN\\Downloads\\2021-01-01~2025-10-31\\html\\ê²°ì¬\",\n",
    "]\n",
    "\n",
    "# DB ì—°ê²° ì •ë³´\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 3306,\n",
    "    'database': 'any_approval',\n",
    "    'user': 'root',\n",
    "    'password': '1234',\n",
    "    'charset': 'utf8mb4'\n",
    "}\n",
    "\n",
    "print(\"ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"HTML í´ë” ìˆ˜: {len(HTML_FOLDERS)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì…€ 2: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸]\n",
      "ë¬¸ì„œë²ˆí˜¸ ì¶”ì¶œ: 23447133\n",
      "ë‚ ì§œâ†’íƒ€ì„ìŠ¤íƒ¬í”„: 1721969680000\n",
      "íƒ€ì„ìŠ¤íƒ¬í”„â†’ë‚ ì§œ: 2024-07-26 13:54:40\n",
      "ì‘ì„±ì ì¶”ì¶œ: ì„í˜„ì¤€\n"
     ]
    }
   ],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"DB ì—°ê²° ìƒì„±\"\"\"\n",
    "    return mysql.connector.connect(**DB_CONFIG)\n",
    "\n",
    "\n",
    "def extract_doc_id_from_filename(filename):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œë²ˆí˜¸ ì¶”ì¶œ\n",
    "    ì˜ˆ: '20240726_[í•˜ê³„íœ´ê°€ 7_29~30] íœ´ê°€ì‹ ì²­_23447133.html' -> '23447133'\n",
    "    \"\"\"\n",
    "    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    # ë§ˆì§€ë§‰ ì–¸ë”ìŠ¤ì½”ì–´ ë’¤ì˜ ìˆ«ì ì¶”ì¶œ\n",
    "    match = re.search(r'_(\\d+)$', name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def datetime_to_timestamp_ms(dt_str):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ ë¬¸ìì—´ì„ Unix timestamp (ë°€ë¦¬ì´ˆ)ë¡œ ë³€í™˜\n",
    "    ì˜ˆ: '2024-07-26 13:54:40' -> 1721969680000\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dt = datetime.strptime(dt_str.strip(), '%Y-%m-%d %H:%M:%S')\n",
    "        return int(dt.timestamp() * 1000)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def timestamp_ms_to_datetime(ts_ms):\n",
    "    \"\"\"\n",
    "    Unix timestamp (ë°€ë¦¬ì´ˆ)ë¥¼ ë‚ ì§œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    ì˜ˆ: 1721969680000 -> '2024-07-26 13:54:40'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dt = datetime.fromtimestamp(ts_ms / 1000)\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def extract_writer_name(writer_full):\n",
    "    \"\"\"\n",
    "    ì‘ì„±ì ì „ì²´ ë¬¸ìì—´ì—ì„œ ì´ë¦„ë§Œ ì¶”ì¶œ\n",
    "    ì˜ˆ: 'ì„í˜„ì¤€/íŒ€ì¥/DXì‚¬ì—…íŒ€' -> 'ì„í˜„ì¤€'\n",
    "    \"\"\"\n",
    "    if not writer_full:\n",
    "        return ''\n",
    "    writer_full = writer_full.strip()\n",
    "    if '/' in writer_full:\n",
    "        return writer_full.split('/')[0].strip()\n",
    "    return writer_full\n",
    "\n",
    "\n",
    "print(\"ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\n[í…ŒìŠ¤íŠ¸]\")\n",
    "print(f\"ë¬¸ì„œë²ˆí˜¸ ì¶”ì¶œ: {extract_doc_id_from_filename('20240726_[í•˜ê³„íœ´ê°€ 7_29~30] íœ´ê°€ì‹ ì²­_23447133.html')}\")\n",
    "print(f\"ë‚ ì§œâ†’íƒ€ì„ìŠ¤íƒ¬í”„: {datetime_to_timestamp_ms('2024-07-26 13:54:40')}\")\n",
    "print(f\"íƒ€ì„ìŠ¤íƒ¬í”„â†’ë‚ ì§œ: {timestamp_ms_to_datetime(1721969680000)}\")\n",
    "print(f\"ì‘ì„±ì ì¶”ì¶œ: {extract_writer_name('ì„í˜„ì¤€/íŒ€ì¥/DXì‚¬ì—…íŒ€')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì…€ 3: HTML íŒŒì‹± í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML íŒŒì‹± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def get_text_with_linebreaks(element):\n",
    "    \"\"\"\n",
    "    HTML ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (br íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜)\n",
    "    <br>, <br/>, <br /> ëª¨ë‘ ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    if element is None:\n",
    "        return ''\n",
    "    \n",
    "    # ë¨¼ì € br íƒœê·¸ë¥¼ íŠ¹ìˆ˜ ë§ˆì»¤ë¡œ ë³€í™˜\n",
    "    for br in element.find_all('br'):\n",
    "        br.replace_with('\\n')\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    text = element.get_text()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def parse_comments_from_html(html_path):\n",
    "    \"\"\"\n",
    "    HTML íŒŒì¼ì—ì„œ ê²°ì¬ëŒ“ê¸€ ì¶”ì¶œ\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: [{'writer': 'ì„í˜„ì¤€', 'created_at': '2024-07-26 13:54:40', 'message': '...'}, ...]\n",
    "        None if error\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        # íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì²˜ë¦¬)\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding='cp949') as f:\n",
    "                html_content = f.read()\n",
    "        except:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    # BeautifulSoupìœ¼ë¡œ íŒŒì‹± (lxml íŒŒì„œ ì‚¬ìš©)\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    \n",
    "    # ê²°ì¬ëŒ“ê¸€ í…Œì´ë¸” ì°¾ê¸°\n",
    "    comment_table = soup.find('table', {'summary': 'ê²°ì¬ëŒ“ê¸€'})\n",
    "    if not comment_table:\n",
    "        return comments  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ëŒ“ê¸€ í…Œì´ë¸” ìì²´ê°€ ì—†ìŒ)\n",
    "    \n",
    "    # td ì•ˆì—ì„œ ëŒ“ê¸€ ì¶”ì¶œ\n",
    "    td = comment_table.find('td')\n",
    "    if not td:\n",
    "        return comments\n",
    "    \n",
    "    # span.F_12_black_b (ì‘ì„±ì)ì™€ span.F_11_gray (ë‚ ì§œ) ìŒì„ ì°¾ê¸°\n",
    "    writer_spans = td.find_all('span', class_='F_12_black_b')\n",
    "    \n",
    "    for writer_span in writer_spans:\n",
    "        comment = {}\n",
    "        \n",
    "        # ì‘ì„±ì ì¶”ì¶œ\n",
    "        writer_full = writer_span.get_text().strip()\n",
    "        comment['writer'] = extract_writer_name(writer_full)\n",
    "        \n",
    "        # ë‚ ì§œ ì¶”ì¶œ (ì‘ì„±ì span ë‹¤ìŒì˜ F_11_gray span)\n",
    "        date_span = writer_span.find_next_sibling('span', class_='F_11_gray')\n",
    "        if date_span:\n",
    "            comment['created_at'] = date_span.get_text().strip()\n",
    "        else:\n",
    "            # ë¶€ëª¨ì—ì„œ ì°¾ê¸°\n",
    "            parent = writer_span.parent\n",
    "            if parent:\n",
    "                date_span = parent.find('span', class_='F_11_gray')\n",
    "                if date_span:\n",
    "                    comment['created_at'] = date_span.get_text().strip()\n",
    "                else:\n",
    "                    comment['created_at'] = ''\n",
    "            else:\n",
    "                comment['created_at'] = ''\n",
    "        \n",
    "        # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì‘ì„±ì span ì´í›„ì˜ div ì•ˆì˜ ë‚´ìš©)\n",
    "        # êµ¬ì¡°: span.user > br > div > (pre ë˜ëŠ” í…ìŠ¤íŠ¸)\n",
    "        message = ''\n",
    "        \n",
    "        # writer_spanì˜ ë¶€ëª¨(span.user)ë¥¼ ì°¾ê³ , ê·¸ ë‹¤ìŒ divë¥¼ ì°¾ê¸°\n",
    "        parent_span = writer_span.parent\n",
    "        if parent_span and parent_span.name == 'span':\n",
    "            # ë‹¤ìŒ í˜•ì œ ìš”ì†Œë“¤ ì¤‘ div ì°¾ê¸°\n",
    "            next_elem = parent_span.next_sibling\n",
    "            while next_elem:\n",
    "                if hasattr(next_elem, 'name') and next_elem.name == 'div':\n",
    "                    # div ì°¾ìŒ, ê·¸ ì•ˆì˜ pre ë˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                    pre = next_elem.find('pre')\n",
    "                    if pre:\n",
    "                        message = get_text_with_linebreaks(pre)\n",
    "                    else:\n",
    "                        message = get_text_with_linebreaks(next_elem)\n",
    "                    break\n",
    "                next_elem = next_elem.next_sibling\n",
    "        \n",
    "        comment['message'] = message\n",
    "        \n",
    "        # ìœ íš¨í•œ ëŒ“ê¸€ë§Œ ì¶”ê°€ (ì‘ì„±ì ë˜ëŠ” ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)\n",
    "        if comment['writer'] or comment['message']:\n",
    "            comments.append(comment)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "print(\"HTML íŒŒì‹± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì…€ 4: HTML íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\n",
      "\n",
      "=== HTML íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ ===\n",
      "ì´ íŒŒì¼ ìˆ˜: 23,306ê°œ\n",
      "\n",
      "í´ë”ë³„:\n",
      "  - 2010-01-01~2010-12-31: 840ê°œ\n",
      "  - 2011-01-01~2015-12-31: 6,587ê°œ\n",
      "  - 2016-01-01~2020-12-31: 7,704ê°œ\n",
      "  - 2021-01-01~2025-10-31: 8,175ê°œ\n"
     ]
    }
   ],
   "source": [
    "def collect_html_files():\n",
    "    \"\"\"\n",
    "    ëª¨ë“  í´ë”ì—ì„œ HTML íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘\n",
    "    \n",
    "    Returns:\n",
    "        dict: {ë¬¸ì„œë²ˆí˜¸: íŒŒì¼ê²½ë¡œ}\n",
    "    \"\"\"\n",
    "    html_files = {}\n",
    "    folder_stats = {}\n",
    "    \n",
    "    for folder in HTML_FOLDERS:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"âš ï¸ í´ë” ì—†ìŒ: {folder}\")\n",
    "            continue\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(os.path.dirname(folder)))\n",
    "        count = 0\n",
    "        \n",
    "        # í•˜ìœ„ ì—°ë„ í´ë”ë“¤ ìˆœíšŒ\n",
    "        for year_folder in glob.glob(os.path.join(folder, '*')):\n",
    "            if os.path.isdir(year_folder):\n",
    "                for html_file in glob.glob(os.path.join(year_folder, '*.html')):\n",
    "                    doc_id = extract_doc_id_from_filename(os.path.basename(html_file))\n",
    "                    if doc_id:\n",
    "                        html_files[doc_id] = html_file\n",
    "                        count += 1\n",
    "        \n",
    "        folder_stats[folder_name] = count\n",
    "    \n",
    "    return html_files, folder_stats\n",
    "\n",
    "\n",
    "# HTML íŒŒì¼ ìˆ˜ì§‘\n",
    "print(\"HTML íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\")\n",
    "html_files_dict, folder_stats = collect_html_files()\n",
    "\n",
    "print(f\"\\n=== HTML íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ ===\")\n",
    "print(f\"ì´ íŒŒì¼ ìˆ˜: {len(html_files_dict):,}ê°œ\\n\")\n",
    "\n",
    "print(\"í´ë”ë³„:\")\n",
    "for folder_name, count in folder_stats.items():\n",
    "    print(f\"  - {folder_name}: {count:,}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì…€ 5: DB ëŒ“ê¸€ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DB comments í…Œì´ë¸” í˜„í™© ===\n",
      "ì´ ë ˆì½”ë“œ: 100ê°œ\n",
      "ë¬¸ì„œ ìˆ˜: 86ê°œ (ì¤‘ë³µ ì œì™¸)\n",
      "\n",
      "âœ… source_id ì¤‘ë³µ ì—†ìŒ\n",
      "\n",
      "ì—°ë„ë³„:\n",
      "  - 2020: 5ê°œ\n",
      "  - 2021: 19ê°œ\n",
      "  - 2022: 14ê°œ\n",
      "  - 2023: 5ê°œ\n",
      "  - 2024: 27ê°œ\n",
      "  - 2025: 30ê°œ\n",
      "\n",
      "ê¸°ê°„: 2020-11-09 11:23:05 ~ 2025-12-16 13:36:17\n"
     ]
    }
   ],
   "source": [
    "# DBì—ì„œ ëŒ“ê¸€ ë°ì´í„° ë¡œë“œ\n",
    "conn = get_db_connection()\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    source_id,\n",
    "    source_document_id,\n",
    "    created_at,\n",
    "    updated_at,\n",
    "    writer,\n",
    "    message\n",
    "FROM comments\n",
    "ORDER BY source_document_id, created_at\n",
    "\"\"\"\n",
    "\n",
    "db_comments_df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€ (ì½ê¸° ì‰½ê²Œ)\n",
    "db_comments_df['created_at_str'] = db_comments_df['created_at'].apply(timestamp_ms_to_datetime)\n",
    "\n",
    "print(f\"=== DB comments í…Œì´ë¸” í˜„í™© ===\")\n",
    "print(f\"ì´ ë ˆì½”ë“œ: {len(db_comments_df)}ê°œ\")\n",
    "print(f\"ë¬¸ì„œ ìˆ˜: {db_comments_df['source_document_id'].nunique()}ê°œ (ì¤‘ë³µ ì œì™¸)\")\n",
    "\n",
    "# ì¤‘ë³µ ì²´í¬ (source_id ê¸°ì¤€)\n",
    "duplicate_source_ids = db_comments_df[db_comments_df.duplicated(subset=['source_id'], keep=False)]\n",
    "if len(duplicate_source_ids) > 0:\n",
    "    print(f\"\\nâš ï¸ source_id ì¤‘ë³µ ë ˆì½”ë“œ ë°œê²¬: {len(duplicate_source_ids)}ê±´\")\n",
    "    print(duplicate_source_ids[['source_id', 'writer', 'created_at_str', 'message']].head(10))\n",
    "else:\n",
    "    print(f\"\\nâœ… source_id ì¤‘ë³µ ì—†ìŒ\")\n",
    "\n",
    "# ì—°ë„ë³„ í†µê³„\n",
    "db_comments_df['year'] = pd.to_datetime(db_comments_df['created_at_str']).dt.year\n",
    "print(f\"\\nì—°ë„ë³„:\")\n",
    "year_counts = db_comments_df.groupby('year').size()\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  - {year}: {count}ê°œ\")\n",
    "\n",
    "print(f\"\\nê¸°ê°„: {db_comments_df['created_at_str'].min()} ~ {db_comments_df['created_at_str'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DB ëŒ“ê¸€ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>writer</th>\n",
       "      <th>created_at_str</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14835264</td>\n",
       "      <td>ìµœê·œ</td>\n",
       "      <td>2020-11-09 11:23:05</td>\n",
       "      <td>ìŠ¹ì¸í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14884099</td>\n",
       "      <td>ë°•ìƒíƒ</td>\n",
       "      <td>2020-11-17 15:52:40</td>\n",
       "      <td>ìµœì¢… ê²°ì¬ìëŠ” ìŠ¹ì¸ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14967986</td>\n",
       "      <td>ìµœê¸°ì›</td>\n",
       "      <td>2020-12-02 10:15:48</td>\n",
       "      <td>ì›ë³¸ ì˜ìˆ˜ì¦ë„ ë³´ë‚´ì£¼ì„¸ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15083828</td>\n",
       "      <td>ì†¡ì¤€ì„ </td>\n",
       "      <td>2020-12-21 09:37:28</td>\n",
       "      <td>- ETRI í†µí•©ì •ë³´ì‹œìŠ¤í…œ ìœ ì§€ê´€ë¦¬ ê´€ë ¨ 11ì›” ë¹„ìš© ì§€ê¸‰ì„ í•©ì˜í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15119275</td>\n",
       "      <td>ì¡°ìœ¤í˜¸</td>\n",
       "      <td>2020-12-24 10:34:13</td>\n",
       "      <td>ê°ì‚¬í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_document_id writer       created_at_str  \\\n",
       "0           14835264     ìµœê·œ  2020-11-09 11:23:05   \n",
       "1           14884099    ë°•ìƒíƒ  2020-11-17 15:52:40   \n",
       "2           14967986    ìµœê¸°ì›  2020-12-02 10:15:48   \n",
       "3           15083828    ì†¡ì¤€ì„   2020-12-21 09:37:28   \n",
       "4           15119275    ì¡°ìœ¤í˜¸  2020-12-24 10:34:13   \n",
       "\n",
       "                                    message  \n",
       "0                                    ìŠ¹ì¸í•©ë‹ˆë‹¤.  \n",
       "1                       ìµœì¢… ê²°ì¬ìëŠ” ìŠ¹ì¸ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤  \n",
       "2                             ì›ë³¸ ì˜ìˆ˜ì¦ë„ ë³´ë‚´ì£¼ì„¸ìš”  \n",
       "3  - ETRI í†µí•©ì •ë³´ì‹œìŠ¤í…œ ìœ ì§€ê´€ë¦¬ ê´€ë ¨ 11ì›” ë¹„ìš© ì§€ê¸‰ì„ í•©ì˜í•©ë‹ˆë‹¤.  \n",
       "4                                    ê°ì‚¬í•©ë‹ˆë‹¤.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DB ëŒ“ê¸€ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"=== DB ëŒ“ê¸€ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ) ===\")\n",
    "db_comments_df[['source_document_id', 'writer', 'created_at_str', 'message']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ” 1ë‹¨ê³„: DB ëŒ“ê¸€ ê¸°ì¤€ HTML íŒŒì‹± ê²€ì¦\n",
    "\n",
    "DBì— ìˆëŠ” 100ê°œ ëŒ“ê¸€ì„ HTMLì—ì„œ íŒŒì‹±í•œ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ íŒŒì‹± ë¡œì§ì´ ì •í™•í•œì§€ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBì— ìˆëŠ” ë¬¸ì„œë²ˆí˜¸: 86ê°œ\n",
      "HTML íŒŒì¼ ìˆëŠ” ë¬¸ì„œ: 85ê°œ\n",
      "HTML íŒŒì¼ ì—†ëŠ” ë¬¸ì„œ: 1ê°œ\n",
      "\n",
      "ğŸ“‹ HTML íŒŒì¼ ì—†ëŠ” ë¬¸ì„œ ëª©ë¡ (DBì— ìˆì§€ë§Œ HTML ì—†ìŒ):\n",
      "  - ë¬¸ì„œë²ˆí˜¸: 26707524, ì‘ì„±ì¼: 2025-12-16 13:36:17, ì‘ì„±ì: ì´ìš©ì¼\n"
     ]
    }
   ],
   "source": [
    "# DBì— ìˆëŠ” ëŒ“ê¸€ì˜ ë¬¸ì„œë²ˆí˜¸ ëª©ë¡\n",
    "db_doc_ids = db_comments_df['source_document_id'].unique().tolist()\n",
    "print(f\"DBì— ìˆëŠ” ë¬¸ì„œë²ˆí˜¸: {len(db_doc_ids)}ê°œ\")\n",
    "\n",
    "# HTML íŒŒì¼ì´ ìˆëŠ” ë¬¸ì„œë²ˆí˜¸ í•„í„°ë§\n",
    "db_doc_ids_with_html = [doc_id for doc_id in db_doc_ids if doc_id in html_files_dict]\n",
    "db_doc_ids_without_html = [doc_id for doc_id in db_doc_ids if doc_id not in html_files_dict]\n",
    "\n",
    "print(f\"HTML íŒŒì¼ ìˆëŠ” ë¬¸ì„œ: {len(db_doc_ids_with_html)}ê°œ\")\n",
    "print(f\"HTML íŒŒì¼ ì—†ëŠ” ë¬¸ì„œ: {len(db_doc_ids_without_html)}ê°œ\")\n",
    "\n",
    "if db_doc_ids_without_html:\n",
    "    print(f\"\\nğŸ“‹ HTML íŒŒì¼ ì—†ëŠ” ë¬¸ì„œ ëª©ë¡ (DBì— ìˆì§€ë§Œ HTML ì—†ìŒ):\")\n",
    "    for doc_id in db_doc_ids_without_html:\n",
    "        doc_comments = db_comments_df[db_comments_df['source_document_id'] == doc_id]\n",
    "        for _, row in doc_comments.iterrows():\n",
    "            print(f\"  - ë¬¸ì„œë²ˆí˜¸: {doc_id}, ì‘ì„±ì¼: {row['created_at_str']}, ì‘ì„±ì: {row['writer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HTML íŒŒì‹± ê²€ì¦ ì‹œì‘ ===\n",
      "ê²€ì¦ ëŒ€ìƒ: 85ê°œ ë¬¸ì„œ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTML íŒŒì‹± ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:03<00:00, 27.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ê²€ì¦ ì™„ë£Œ: 99ê°œ ëŒ“ê¸€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HTMLì—ì„œ ëŒ“ê¸€ íŒŒì‹± ë° DBì™€ ë¹„êµ\n",
    "print(\"=== HTML íŒŒì‹± ê²€ì¦ ì‹œì‘ ===\")\n",
    "print(f\"ê²€ì¦ ëŒ€ìƒ: {len(db_doc_ids_with_html)}ê°œ ë¬¸ì„œ\\n\")\n",
    "\n",
    "validation_results = []\n",
    "parse_errors = []\n",
    "\n",
    "for doc_id in tqdm(db_doc_ids_with_html, desc=\"HTML íŒŒì‹± ì¤‘\"):\n",
    "    html_path = html_files_dict[doc_id]\n",
    "    \n",
    "    # HTMLì—ì„œ ëŒ“ê¸€ íŒŒì‹±\n",
    "    html_comments = parse_comments_from_html(html_path)\n",
    "    \n",
    "    if html_comments is None:\n",
    "        parse_errors.append({'doc_id': doc_id, 'error': 'íŒŒì¼ ì½ê¸° ì‹¤íŒ¨', 'path': html_path})\n",
    "        continue\n",
    "    \n",
    "    # DBì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°\n",
    "    db_doc_comments = db_comments_df[db_comments_df['source_document_id'] == doc_id].copy()\n",
    "    \n",
    "    # ë¹„êµ\n",
    "    for idx, db_row in db_doc_comments.iterrows():\n",
    "        result = {\n",
    "            'doc_id': doc_id,\n",
    "            'db_writer': db_row['writer'],\n",
    "            'db_created_at': db_row['created_at_str'],\n",
    "            'db_message': db_row['message'],\n",
    "            'html_writer': None,\n",
    "            'html_created_at': None,\n",
    "            'html_message': None,\n",
    "            'match_status': 'NOT_FOUND',\n",
    "            'detail': ''\n",
    "        }\n",
    "        \n",
    "        # HTML ëŒ“ê¸€ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸° (ì‘ì„±ì + ë‚ ì§œ ê¸°ì¤€)\n",
    "        for html_comment in html_comments:\n",
    "            if (html_comment['writer'] == db_row['writer'] and \n",
    "                html_comment['created_at'] == db_row['created_at_str']):\n",
    "                \n",
    "                result['html_writer'] = html_comment['writer']\n",
    "                result['html_created_at'] = html_comment['created_at']\n",
    "                result['html_message'] = html_comment['message']\n",
    "                \n",
    "                # ë‚´ìš© ë¹„êµ\n",
    "                db_msg = (db_row['message'] or '').strip()\n",
    "                html_msg = (html_comment['message'] or '').strip()\n",
    "                \n",
    "                if db_msg == html_msg:\n",
    "                    result['match_status'] = 'EXACT_MATCH'\n",
    "                elif db_msg.replace('\\r\\n', '\\n') == html_msg.replace('\\r\\n', '\\n'):\n",
    "                    result['match_status'] = 'MATCH_CRLF_DIFF'\n",
    "                    result['detail'] = 'ì¤„ë°”ê¿ˆ ë¬¸ì ì°¨ì´ (\\\\r\\\\n vs \\\\n)'\n",
    "                elif db_msg.replace(' ', '').replace('\\n', '') == html_msg.replace(' ', '').replace('\\n', ''):\n",
    "                    result['match_status'] = 'MATCH_WHITESPACE_DIFF'\n",
    "                    result['detail'] = 'ê³µë°±/ì¤„ë°”ê¿ˆ ì°¨ì´'\n",
    "                else:\n",
    "                    result['match_status'] = 'CONTENT_MISMATCH'\n",
    "                    result['detail'] = f'DBê¸¸ì´:{len(db_msg)}, HTMLê¸¸ì´:{len(html_msg)}'\n",
    "                break\n",
    "        \n",
    "        validation_results.append(result)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(f\"\\nê²€ì¦ ì™„ë£Œ: {len(validation_df)}ê°œ ëŒ“ê¸€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== íŒŒì‹± ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===\n",
      "  âœ… ì™„ì „ ì¼ì¹˜: 99ê°œ (100.0%)\n",
      "\n",
      "ğŸ“Š ì „ì²´ ì¼ì¹˜ìœ¨: 100.0% (99/99)\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ ê²°ê³¼ ìš”ì•½\n",
    "print(\"=== íŒŒì‹± ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===\")\n",
    "\n",
    "status_counts = validation_df['match_status'].value_counts()\n",
    "total = len(validation_df)\n",
    "\n",
    "for status, count in status_counts.items():\n",
    "    pct = count / total * 100\n",
    "    if status == 'EXACT_MATCH':\n",
    "        print(f\"  âœ… ì™„ì „ ì¼ì¹˜: {count}ê°œ ({pct:.1f}%)\")\n",
    "    elif status == 'MATCH_CRLF_DIFF':\n",
    "        print(f\"  âš ï¸ ì¤„ë°”ê¿ˆ ë¬¸ì ì°¨ì´ë§Œ ìˆìŒ: {count}ê°œ ({pct:.1f}%)\")\n",
    "    elif status == 'MATCH_WHITESPACE_DIFF':\n",
    "        print(f\"  âš ï¸ ê³µë°±/ì¤„ë°”ê¿ˆ ì°¨ì´ë§Œ ìˆìŒ: {count}ê°œ ({pct:.1f}%)\")\n",
    "    elif status == 'CONTENT_MISMATCH':\n",
    "        print(f\"  âŒ ë‚´ìš© ë¶ˆì¼ì¹˜: {count}ê°œ ({pct:.1f}%)\")\n",
    "    elif status == 'NOT_FOUND':\n",
    "        print(f\"  â“ HTMLì—ì„œ ëª» ì°¾ìŒ: {count}ê°œ ({pct:.1f}%)\")\n",
    "\n",
    "if parse_errors:\n",
    "    print(f\"\\nâš ï¸ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {len(parse_errors)}ê±´\")\n",
    "    for err in parse_errors:\n",
    "        print(f\"  - ë¬¸ì„œë²ˆí˜¸: {err['doc_id']}, ì˜¤ë¥˜: {err['error']}\")\n",
    "\n",
    "# ì¼ì¹˜ìœ¨ ê³„ì‚°\n",
    "match_count = len(validation_df[validation_df['match_status'].isin(['EXACT_MATCH', 'MATCH_CRLF_DIFF', 'MATCH_WHITESPACE_DIFF'])])\n",
    "match_rate = match_count / total * 100 if total > 0 else 0\n",
    "print(f\"\\nğŸ“Š ì „ì²´ ì¼ì¹˜ìœ¨: {match_rate:.1f}% ({match_count}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ëŒ“ê¸€ì´ ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ë¶ˆì¼ì¹˜/ë¯¸ë°œê²¬ ì¼€ì´ìŠ¤ ìƒì„¸ í™•ì¸\n",
    "problem_cases = validation_df[~validation_df['match_status'].isin(['EXACT_MATCH', 'MATCH_CRLF_DIFF'])]\n",
    "\n",
    "if len(problem_cases) > 0:\n",
    "    print(f\"=== ë¶ˆì¼ì¹˜/ë¯¸ë°œê²¬ ì¼€ì´ìŠ¤ ìƒì„¸ ({len(problem_cases)}ê±´) ===\")\n",
    "    \n",
    "    for idx, row in problem_cases.iterrows():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[ë¬¸ì„œë²ˆí˜¸: {row['doc_id']}] ìƒíƒœ: {row['match_status']}\")\n",
    "        print(f\"ìƒì„¸: {row['detail']}\")\n",
    "        print(f\"\\nDB ëŒ“ê¸€:\")\n",
    "        print(f\"  ì‘ì„±ì: {row['db_writer']}\")\n",
    "        print(f\"  ë‚ ì§œ: {row['db_created_at']}\")\n",
    "        print(f\"  ë‚´ìš©: {repr(row['db_message'][:200] if row['db_message'] else '')}...\")\n",
    "        print(f\"\\nHTML ëŒ“ê¸€:\")\n",
    "        print(f\"  ì‘ì„±ì: {row['html_writer']}\")\n",
    "        print(f\"  ë‚ ì§œ: {row['html_created_at']}\")\n",
    "        print(f\"  ë‚´ìš©: {repr(row['html_message'][:200] if row['html_message'] else '')}...\")\n",
    "else:\n",
    "    print(\"âœ… ëª¨ë“  ëŒ“ê¸€ì´ ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1ë‹¨ê³„ ê²€ì¦ ê²°ë¡ \n",
      "============================================================\n",
      "\n",
      "ğŸ‰ ì™„ë²½! ëª¨ë“  ëŒ“ê¸€(99ê°œ)ì´ 100% ì™„ì „ ì¼ì¹˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "â†’ 2ë‹¨ê³„ (ì „ì²´ HTML ìŠ¤ìº”)ë¥¼ ì§„í–‰í•´ë„ ë©ë‹ˆë‹¤.\n",
      "\n",
      "VALIDATION_PASSED = True\n"
     ]
    }
   ],
   "source": [
    "# 1ë‹¨ê³„ ê²€ì¦ ê²°ë¡ \n",
    "exact_match_count = len(validation_df[validation_df['match_status'] == 'EXACT_MATCH'])\n",
    "acceptable_match_count = len(validation_df[validation_df['match_status'].isin(['EXACT_MATCH', 'MATCH_CRLF_DIFF'])])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"1ë‹¨ê³„ ê²€ì¦ ê²°ë¡ \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if exact_match_count == total:\n",
    "    print(f\"\\nğŸ‰ ì™„ë²½! ëª¨ë“  ëŒ“ê¸€({total}ê°œ)ì´ 100% ì™„ì „ ì¼ì¹˜í•©ë‹ˆë‹¤.\")\n",
    "    print(\"\\nâ†’ 2ë‹¨ê³„ (ì „ì²´ HTML ìŠ¤ìº”)ë¥¼ ì§„í–‰í•´ë„ ë©ë‹ˆë‹¤.\")\n",
    "    VALIDATION_PASSED = True\n",
    "elif acceptable_match_count == total:\n",
    "    print(f\"\\nâœ… ëª¨ë“  ëŒ“ê¸€({total}ê°œ)ì´ ì¼ì¹˜í•©ë‹ˆë‹¤. (ì¼ë¶€ ì¤„ë°”ê¿ˆ ë¬¸ì ì°¨ì´ ìˆìŒ)\")\n",
    "    print(\"\\nâ†’ 2ë‹¨ê³„ (ì „ì²´ HTML ìŠ¤ìº”)ë¥¼ ì§„í–‰í•´ë„ ë©ë‹ˆë‹¤.\")\n",
    "    VALIDATION_PASSED = True\n",
    "else:\n",
    "    problem_count = total - acceptable_match_count\n",
    "    print(f\"\\nâš ï¸ {problem_count}ê°œ ëŒ“ê¸€ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"\\nâ†’ ìœ„ì˜ ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ë¥¼ í™•ì¸í•˜ê³  íŒŒì‹± ë¡œì§ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    VALIDATION_PASSED = False\n",
    "\n",
    "print(f\"\\nVALIDATION_PASSED = {VALIDATION_PASSED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“‚ 2ë‹¨ê³„: ì „ì²´ HTML ìŠ¤ìº” ë° ëˆ„ë½ ëŒ“ê¸€ ì¶”ì¶œ\n",
    "\n",
    "âš ï¸ **1ë‹¨ê³„ ê²€ì¦ì´ í†µê³¼í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•˜ì„¸ìš”!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 1ë‹¨ê³„ ê²€ì¦ í†µê³¼! 2ë‹¨ê³„ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ í†µê³¼ ì—¬ë¶€ í™•ì¸\n",
    "if not VALIDATION_PASSED:\n",
    "    print(\"âŒ 1ë‹¨ê³„ ê²€ì¦ì´ í†µê³¼í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"íŒŒì‹± ë¡œì§ì„ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âœ… 1ë‹¨ê³„ ê²€ì¦ í†µê³¼! 2ë‹¨ê³„ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì „ì²´ HTML íŒŒì¼ ìŠ¤ìº” ì‹œì‘ ===\n",
      "ìŠ¤ìº” ëŒ€ìƒ: 23,306ê°œ íŒŒì¼\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTML ìŠ¤ìº” ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23306/23306 [11:53<00:00, 32.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HTML ìŠ¤ìº” ê²°ê³¼ ===\n",
      "ì´ HTML íŒŒì¼: 23,306ê°œ\n",
      "â”œâ”€ ëŒ“ê¸€ ìˆëŠ” íŒŒì¼: 85ê°œ (0.4%)\n",
      "â””â”€ ëŒ“ê¸€ ì—†ëŠ” íŒŒì¼: 23,221ê°œ (99.6%)\n",
      "\n",
      "ì´ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜: 99ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ HTML íŒŒì¼ì—ì„œ ëŒ“ê¸€ ì¶”ì¶œ\n",
    "if VALIDATION_PASSED:\n",
    "    print(\"=== ì „ì²´ HTML íŒŒì¼ ìŠ¤ìº” ì‹œì‘ ===\")\n",
    "    print(f\"ìŠ¤ìº” ëŒ€ìƒ: {len(html_files_dict):,}ê°œ íŒŒì¼\\n\")\n",
    "    \n",
    "    all_html_comments = {}  # {doc_id: [comments]}\n",
    "    files_with_comments = 0\n",
    "    files_without_comments = 0\n",
    "    scan_errors = []\n",
    "    year_stats = defaultdict(lambda: {'total': 0, 'with_comments': 0, 'comment_count': 0})\n",
    "    \n",
    "    for doc_id, html_path in tqdm(html_files_dict.items(), desc=\"HTML ìŠ¤ìº” ì¤‘\"):\n",
    "        # ì—°ë„ ì¶”ì¶œ (íŒŒì¼ ê²½ë¡œì—ì„œ)\n",
    "        year_match = re.search(r'\\\\(\\d{4})\\\\', html_path)\n",
    "        year = year_match.group(1) if year_match else 'unknown'\n",
    "        year_stats[year]['total'] += 1\n",
    "        \n",
    "        # HTML íŒŒì‹±\n",
    "        comments = parse_comments_from_html(html_path)\n",
    "        \n",
    "        if comments is None:\n",
    "            scan_errors.append({'doc_id': doc_id, 'path': html_path})\n",
    "            continue\n",
    "        \n",
    "        if comments:\n",
    "            all_html_comments[doc_id] = comments\n",
    "            files_with_comments += 1\n",
    "            year_stats[year]['with_comments'] += 1\n",
    "            year_stats[year]['comment_count'] += len(comments)\n",
    "        else:\n",
    "            files_without_comments += 1\n",
    "    \n",
    "    total_comments = sum(len(c) for c in all_html_comments.values())\n",
    "    \n",
    "    print(f\"\\n=== HTML ìŠ¤ìº” ê²°ê³¼ ===\")\n",
    "    print(f\"ì´ HTML íŒŒì¼: {len(html_files_dict):,}ê°œ\")\n",
    "    print(f\"â”œâ”€ ëŒ“ê¸€ ìˆëŠ” íŒŒì¼: {files_with_comments:,}ê°œ ({files_with_comments/len(html_files_dict)*100:.1f}%)\")\n",
    "    print(f\"â””â”€ ëŒ“ê¸€ ì—†ëŠ” íŒŒì¼: {files_without_comments:,}ê°œ ({files_without_comments/len(html_files_dict)*100:.1f}%)\")\n",
    "    print(f\"\\nì´ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜: {total_comments:,}ê°œ\")\n",
    "    \n",
    "    if scan_errors:\n",
    "        print(f\"\\nâš ï¸ ìŠ¤ìº” ì˜¤ë¥˜: {len(scan_errors)}ê±´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì—°ë„ë³„ ëŒ“ê¸€ í˜„í™© ===\n",
      "ì—°ë„            ì´íŒŒì¼       ëŒ“ê¸€ìˆìŒ       ë¹„ìœ¨        ëŒ“ê¸€ìˆ˜\n",
      "--------------------------------------------------\n",
      "2010          840          0     0.0%          0\n",
      "2011          855          0     0.0%          0\n",
      "2012        1,280          0     0.0%          0\n",
      "2013        1,585          0     0.0%          0\n",
      "2014        1,502          0     0.0%          0\n",
      "2015        1,365          0     0.0%          0\n",
      "2016        1,527          0     0.0%          0\n",
      "2017        1,963          0     0.0%          0\n",
      "2018        1,661          0     0.0%          0\n",
      "2019        1,478          0     0.0%          0\n",
      "2020        1,075          5     0.5%          5\n",
      "2021        1,331         19     1.4%         19\n",
      "2022        1,818         13     0.7%         14\n",
      "2023        1,442          6     0.4%          6\n",
      "2024        1,811         22     1.2%         26\n",
      "2025        1,773         20     1.1%         29\n"
     ]
    }
   ],
   "source": [
    "# ì—°ë„ë³„ ëŒ“ê¸€ í˜„í™©\n",
    "if VALIDATION_PASSED:\n",
    "    print(\"=== ì—°ë„ë³„ ëŒ“ê¸€ í˜„í™© ===\")\n",
    "    print(f\"{'ì—°ë„':<6} {'ì´íŒŒì¼':>10} {'ëŒ“ê¸€ìˆìŒ':>10} {'ë¹„ìœ¨':>8} {'ëŒ“ê¸€ìˆ˜':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for year in sorted(year_stats.keys()):\n",
    "        stats = year_stats[year]\n",
    "        rate = stats['with_comments'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "        print(f\"{year:<6} {stats['total']:>10,} {stats['with_comments']:>10,} {rate:>7.1f}% {stats['comment_count']:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010~2019ë…„ ì¤‘ ê²°ì¬ëŒ“ê¸€ ë‚´ìš©ì´ ìˆëŠ” íŒŒì¼: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# 2010~2019ë…„ HTML ì¤‘ ê²°ì¬ëŒ“ê¸€ tdì— ë‚´ìš©ì´ ìˆëŠ” íŒŒì¼ ì°¾ê¸°\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "check_years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
    "found_files = []\n",
    "\n",
    "for doc_id, html_path in html_files_dict.items():\n",
    "    # 2010~2019ë…„ íŒŒì¼ë§Œ\n",
    "    year_match = re.search(r'\\\\(\\d{4})\\\\', html_path)\n",
    "    if not year_match or year_match.group(1) not in check_years:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'lxml')\n",
    "        \n",
    "        comment_table = soup.find('table', {'summary': 'ê²°ì¬ëŒ“ê¸€'})\n",
    "        if comment_table:\n",
    "            td = comment_table.find('td')\n",
    "            if td:\n",
    "                # td ì•ˆì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ê³µë°± ì œê±°)\n",
    "                text = td.get_text(strip=True)\n",
    "                if len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´\n",
    "                    found_files.append({\n",
    "                        'doc_id': doc_id,\n",
    "                        'year': year_match.group(1),\n",
    "                        'path': html_path,\n",
    "                        'text_preview': text[:100]\n",
    "                    })\n",
    "                    if len(found_files) >= 5:  # 5ê°œë§Œ ì°¾ìœ¼ë©´ ì¤‘ë‹¨\n",
    "                        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"2010~2019ë…„ ì¤‘ ê²°ì¬ëŒ“ê¸€ ë‚´ìš©ì´ ìˆëŠ” íŒŒì¼: {len(found_files)}ê°œ\")\n",
    "for f in found_files:\n",
    "    print(f\"\\nì—°ë„: {f['year']}, ë¬¸ì„œë²ˆí˜¸: {f['doc_id']}\")\n",
    "    print(f\"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {f['text_preview']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DBì™€ HTML ë¹„êµ ===\n",
      "\n",
      "[ë¬¸ì„œ ë‹¨ìœ„ ë¹„êµ]\n",
      "HTMLì—ì„œ ëŒ“ê¸€ ìˆëŠ” ë¬¸ì„œ: 85ê°œ\n",
      "DBì— ìˆëŠ” ë¬¸ì„œ: 86ê°œ\n",
      "â”œâ”€ êµì§‘í•© (ë‘˜ ë‹¤ ìˆìŒ): 85ê°œ\n",
      "â”œâ”€ HTMLì—ë§Œ ìˆìŒ (DB ëˆ„ë½): 0ê°œ  â† INSERT ëŒ€ìƒ\n",
      "â””â”€ DBì—ë§Œ ìˆìŒ (HTMLì— ì—†ìŒ): 1ê°œ\n"
     ]
    }
   ],
   "source": [
    "# DBì™€ ë¹„êµí•˜ì—¬ ëˆ„ë½ ëŒ“ê¸€ ì°¾ê¸°\n",
    "if VALIDATION_PASSED:\n",
    "    print(\"=== DBì™€ HTML ë¹„êµ ===\")\n",
    "    \n",
    "    # HTMLì—ì„œ ì¶”ì¶œí•œ ë¬¸ì„œë²ˆí˜¸\n",
    "    html_doc_ids_with_comments = set(all_html_comments.keys())\n",
    "    \n",
    "    # DBì— ìˆëŠ” ë¬¸ì„œë²ˆí˜¸\n",
    "    db_doc_ids_set = set(db_comments_df['source_document_id'].unique())\n",
    "    \n",
    "    # ë¹„êµ\n",
    "    both = html_doc_ids_with_comments & db_doc_ids_set\n",
    "    html_only = html_doc_ids_with_comments - db_doc_ids_set\n",
    "    db_only = db_doc_ids_set - html_doc_ids_with_comments\n",
    "    \n",
    "    print(f\"\\n[ë¬¸ì„œ ë‹¨ìœ„ ë¹„êµ]\")\n",
    "    print(f\"HTMLì—ì„œ ëŒ“ê¸€ ìˆëŠ” ë¬¸ì„œ: {len(html_doc_ids_with_comments):,}ê°œ\")\n",
    "    print(f\"DBì— ìˆëŠ” ë¬¸ì„œ: {len(db_doc_ids_set)}ê°œ\")\n",
    "    print(f\"â”œâ”€ êµì§‘í•© (ë‘˜ ë‹¤ ìˆìŒ): {len(both)}ê°œ\")\n",
    "    print(f\"â”œâ”€ HTMLì—ë§Œ ìˆìŒ (DB ëˆ„ë½): {len(html_only):,}ê°œ  â† INSERT ëŒ€ìƒ\")\n",
    "    print(f\"â””â”€ DBì—ë§Œ ìˆìŒ (HTMLì— ì—†ìŒ): {len(db_only)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DBì— ìˆì§€ë§Œ HTMLì— ì—†ëŠ” ëŒ“ê¸€ (í™•ì¸ìš©) ===\n",
      "ì´ 1ê°œ ë¬¸ì„œ\n",
      "\n",
      "ë¬¸ì„œë²ˆí˜¸         ì‘ì„±ì¼                  ì‘ì„±ì       \n",
      "---------------------------------------------\n",
      "26707524     2025-12-16 13:36:17  ì´ìš©ì¼       \n"
     ]
    }
   ],
   "source": [
    "# DBì—ë§Œ ìˆëŠ” ëŒ“ê¸€ ìƒì„¸ (í™•ì¸ìš©)\n",
    "if VALIDATION_PASSED and db_only:\n",
    "    print(\"=== DBì— ìˆì§€ë§Œ HTMLì— ì—†ëŠ” ëŒ“ê¸€ (í™•ì¸ìš©) ===\")\n",
    "    print(f\"ì´ {len(db_only)}ê°œ ë¬¸ì„œ\\n\")\n",
    "    \n",
    "    db_only_comments = db_comments_df[db_comments_df['source_document_id'].isin(db_only)]\n",
    "    print(f\"{'ë¬¸ì„œë²ˆí˜¸':<12} {'ì‘ì„±ì¼':<20} {'ì‘ì„±ì':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for _, row in db_only_comments.iterrows():\n",
    "        print(f\"{row['source_document_id']:<12} {row['created_at_str']:<20} {row['writer']:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëˆ„ë½ ëŒ“ê¸€ ë°ì´í„° ì¤€ë¹„ (INSERT ëŒ€ìƒ)\n",
    "if VALIDATION_PASSED:\n",
    "    print(\"=== INSERT ëŒ€ìƒ ë°ì´í„° ì¤€ë¹„ ===\")\n",
    "    \n",
    "    missing_comments = []\n",
    "    \n",
    "    for doc_id in html_only:\n",
    "        comments = all_html_comments[doc_id]\n",
    "        \n",
    "        for idx, comment in enumerate(comments, 1):\n",
    "            # source_id ìƒì„±: ìˆœë²ˆ_ë¬¸ì„œë²ˆí˜¸_01\n",
    "            source_id = f\"{idx:02d}_{doc_id}_01\"\n",
    "            \n",
    "            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜\n",
    "            created_at_ms = datetime_to_timestamp_ms(comment['created_at'])\n",
    "            \n",
    "            missing_comments.append({\n",
    "                'source_id': source_id,\n",
    "                'source_document_id': doc_id,\n",
    "                'created_at': created_at_ms,\n",
    "                'updated_at': created_at_ms,\n",
    "                'writer': comment['writer'],\n",
    "                'message': comment['message'],\n",
    "                'created_at_str': comment['created_at']  # í™•ì¸ìš©\n",
    "            })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing_comments)\n",
    "    \n",
    "    print(f\"ì´ {len(html_only):,}ê°œ ë¬¸ì„œì—ì„œ {len(missing_df):,}ê°œ ëŒ“ê¸€\")\n",
    "    \n",
    "    # ì—°ë„ë³„ ëˆ„ë½ í˜„í™©\n",
    "    if len(missing_df) > 0:\n",
    "        missing_df['year'] = pd.to_datetime(missing_df['created_at_str'], errors='coerce').dt.year\n",
    "        print(f\"\\nì—°ë„ë³„ ëˆ„ë½ í˜„í™©:\")\n",
    "        year_missing = missing_df.groupby('year').size()\n",
    "        for year, count in year_missing.items():\n",
    "            print(f\"  - {int(year) if pd.notna(year) else 'N/A'}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëˆ„ë½ ëŒ“ê¸€ ìƒ˜í”Œ í™•ì¸\n",
    "if VALIDATION_PASSED and len(missing_df) > 0:\n",
    "    print(\"=== ëˆ„ë½ ëŒ“ê¸€ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ) ===\")\n",
    "    display(missing_df[['source_id', 'source_document_id', 'created_at_str', 'writer', 'message']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ë¡œ ì €ì¥ (ë°±ì—…ìš©)\n",
    "if VALIDATION_PASSED and len(missing_df) > 0:\n",
    "    csv_path = 'missing_comments.csv'\n",
    "    missing_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_path}\")\n",
    "    print(f\"   ì´ {len(missing_df):,}ê°œ ë ˆì½”ë“œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ’¾ 3ë‹¨ê³„: DB INSERT\n",
    "\n",
    "âš ï¸ **ì‹ ì¤‘í•˜ê²Œ ì‹¤í–‰í•˜ì„¸ìš”! ì‹¤í–‰ ì „ ìœ„ì˜ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT ì „ ìµœì¢… í™•ì¸\n",
    "if VALIDATION_PASSED and len(missing_df) > 0:\n",
    "    print(\"=== INSERT ì „ ìµœì¢… í™•ì¸ ===\")\n",
    "    print(f\"INSERT ëŒ€ìƒ: {len(missing_df):,}ê°œ ëŒ“ê¸€\")\n",
    "    print(f\"\\nâš ï¸ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ DBì— ë°ì´í„°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.\")\n",
    "    print(f\"   ì‹¤í–‰ ì „ ìœ„ì˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"INSERTí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB INSERT ì‹¤í–‰\n",
    "# âš ï¸ ì£¼ì˜: ì´ ì…€ì€ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!\n",
    "\n",
    "DO_INSERT = False  # Trueë¡œ ë³€ê²½í•˜ë©´ ì‹¤í–‰ë¨\n",
    "\n",
    "if DO_INSERT and VALIDATION_PASSED and len(missing_df) > 0:\n",
    "    print(\"=== DB INSERT ì‹œì‘ ===\")\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO comments (source_id, source_document_id, created_at, updated_at, writer, message)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    success_count = 0\n",
    "    skip_count = 0\n",
    "    error_count = 0\n",
    "    errors = []\n",
    "    \n",
    "    for _, row in tqdm(missing_df.iterrows(), total=len(missing_df), desc=\"INSERT ì¤‘\"):\n",
    "        try:\n",
    "            cursor.execute(insert_query, (\n",
    "                row['source_id'],\n",
    "                row['source_document_id'],\n",
    "                row['created_at'],\n",
    "                row['updated_at'],\n",
    "                row['writer'],\n",
    "                row['message']\n",
    "            ))\n",
    "            success_count += 1\n",
    "        except mysql.connector.IntegrityError as e:\n",
    "            # source_id ì¤‘ë³µ (ì´ë¯¸ ì¡´ì¬)\n",
    "            skip_count += 1\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            errors.append({'source_id': row['source_id'], 'error': str(e)})\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n=== INSERT ê²°ê³¼ ===\")\n",
    "    print(f\"âœ… ì„±ê³µ: {success_count:,}ê°œ\")\n",
    "    print(f\"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {skip_count}ê°œ\")\n",
    "    print(f\"âŒ ì‹¤íŒ¨: {error_count}ê°œ\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nì˜¤ë¥˜ ìƒì„¸:\")\n",
    "        for err in errors[:10]:\n",
    "            print(f\"  - {err['source_id']}: {err['error']}\")\n",
    "else:\n",
    "    if not DO_INSERT:\n",
    "        print(\"DO_INSERT = False ì…ë‹ˆë‹¤.\")\n",
    "        print(\"INSERTë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ìœ„ì—ì„œ DO_INSERT = True ë¡œ ë³€ê²½í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT í›„ í™•ì¸\n",
    "if DO_INSERT and VALIDATION_PASSED:\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT COUNT(*) as cnt FROM comments\"\n",
    "    result = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"=== INSERT í›„ DB í˜„í™© ===\")\n",
    "    print(f\"comments í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {result['cnt'].values[0]:,}ê°œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
