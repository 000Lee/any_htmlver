{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641c11ed-9607-4a53-868f-33cbf92e2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 840ê°œì˜ HTML íŒŒì¼ ìŠ¤ìº” ì¤‘...\n",
      "\n",
      "ì§„í–‰: 100/840 (ì¤‘ë³µ ë°œê²¬: 1ê±´)\n",
      "ì§„í–‰: 200/840 (ì¤‘ë³µ ë°œê²¬: 3ê±´)\n",
      "ì§„í–‰: 300/840 (ì¤‘ë³µ ë°œê²¬: 5ê±´)\n",
      "ì§„í–‰: 400/840 (ì¤‘ë³µ ë°œê²¬: 5ê±´)\n",
      "ì§„í–‰: 500/840 (ì¤‘ë³µ ë°œê²¬: 6ê±´)\n",
      "ì§„í–‰: 600/840 (ì¤‘ë³µ ë°œê²¬: 6ê±´)\n",
      "ì§„í–‰: 700/840 (ì¤‘ë³µ ë°œê²¬: 6ê±´)\n",
      "ì§„í–‰: 800/840 (ì¤‘ë³µ ë°œê²¬: 6ê±´)\n",
      "\n",
      "============================================================\n",
      "ğŸ” ìŠ¤ìº” ì™„ë£Œ: 840ê°œ íŒŒì¼\n",
      "âš ï¸  ì¤‘ë³µ ë°œê²¬: 6ê°œ íŒŒì¼\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ ì¤‘ë³µ ì˜ê²¬ì´ ìˆëŠ” íŒŒì¼ ëª©ë¡:\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100220_ì•„ì›ƒì†Œì‹± ì¸ì› ê³„ì•½ ì§„í–‰ í’ˆì˜-í•œêµ­í•œì˜í•™ì—°êµ¬ì›(ë°•ê¸°ì˜,ì˜¤ê²½ì§„)_2002153.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100220_ì•„ì›ƒì†Œì‹± ì¸ì› ê³„ì•½ ì§„í–‰ í’ˆì˜-í•œêµ­í•œì˜í•™ì—°êµ¬ì›(ë°•ê¸°ì˜,ì˜¤ê²½ì§„)_2002153.html\n",
      "   ğŸ‘¤ CEO: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['ê¹€í•˜ì‘', 'ê³ í™”ì„', 'CEO', 'CEO', 'CEO', 'CEO']\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100303_ì•„ì›ƒì†Œì‹± ì¸ê±´ë¹„ ë¹„ìš© ê²°ì œ í’ˆì˜-í•œêµ­ì „ê¸°ì—°êµ¬ì›(ì´ë™ì–¸)_2002182.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100303_ì•„ì›ƒì†Œì‹± ì¸ê±´ë¹„ ë¹„ìš© ê²°ì œ í’ˆì˜-í•œêµ­ì „ê¸°ì—°êµ¬ì›(ì´ë™ì–¸)_2002182.html\n",
      "   ğŸ‘¤ CEO: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['CEO', 'CEO', 'CEO', 'CEO']\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100331_ì‹ì•½ì²­ í”„ë¡œì íŠ¸ ë¹„ìš©ì§€ê¸‰ í’ˆì˜_2002265.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100331_ì‹ì•½ì²­ í”„ë¡œì íŠ¸ ë¹„ìš©ì§€ê¸‰ í’ˆì˜_2002265.html\n",
      "   ğŸ‘¤ CEO: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['ê¹€ë™ë ¥', 'CEO', 'CEO', 'CEO', 'CEO']\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100408_ë©”ë””ìŠ¨ ì§€ì¬ê¶Œí†µí•©ê´€ë¦¬ì‹œìŠ¤í…œ_í•˜ë“œì›¨ì–´ ë°œì£¼ í’ˆì˜_2002281.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100408_ë©”ë””ìŠ¨ ì§€ì¬ê¶Œí†µí•©ê´€ë¦¬ì‹œìŠ¤í…œ_í•˜ë“œì›¨ì–´ ë°œì£¼ í’ˆì˜_2002281.html\n",
      "   ğŸ‘¤ CEO: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['ê¹€ë™ë ¥', 'ê¹€ê¸°ë‘', 'CEO', 'CEO', 'CEO', 'CEO']\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100430_í•œêµ­íƒ€ì´ì–´ ë¬¸ì„œë³´ì•ˆ ìœ ì§€ë³´ìˆ˜_2002345.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100430_í•œêµ­íƒ€ì´ì–´ ë¬¸ì„œë³´ì•ˆ ìœ ì§€ë³´ìˆ˜_2002345.html\n",
      "   ğŸ‘¤ CEO: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['ê¹€ê¸°ë‘', 'CEO', 'CEO', 'CEO', 'CEO']\n",
      "\n",
      "ğŸ“„ íŒŒì¼: 20100802_LGí™”í•™ íŠ¹í—ˆë¶„ì„ë¦¬ìŠ¤í¬ê´€ë¦¬í”„ë¡œì íŠ¸ ì™¸ì£¼ê°œë°œì ì†Œì‹± ìš”ì²­(ê²€ìƒ‰ì—”ì§„ì „ë¬¸ê°€)_2002536.html\n",
      "   ê²½ë¡œ: C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬\\2010\\20100802_LGí™”í•™ íŠ¹í—ˆë¶„ì„ë¦¬ìŠ¤í¬ê´€ë¦¬í”„ë¡œì íŠ¸ ì™¸ì£¼ê°œë°œì ì†Œì‹± ìš”ì²­(ê²€ìƒ‰ì—”ì§„ì „ë¬¸ê°€)_2002536.html\n",
      "   ğŸ‘¤ ê¹€ë™ë ¥: 4ë²ˆ ë“±ì¥\n",
      "   ì „ì²´: ['ê¹€ë™ë ¥', 'ê¹€ë™ë ¥', 'ê¹€ë™ë ¥', 'ê¹€ë™ë ¥', 'CEO']\n",
      "\n",
      "ğŸ“ ê²°ê³¼ê°€ 'duplicate_comments.txt'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def extract_person_info(text):\n",
    "    \"\"\"ì´ë¦„/ì§ì±…/ë¶€ì„œ í˜•ì‹ì—ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    text = re.sub(r'\\d+', '', text).strip()\n",
    "    parts = text.split('/')\n",
    "    if len(parts) >= 3:\n",
    "        return {\n",
    "            'name': parts[0].strip(),\n",
    "            'positionName': parts[1].strip(),\n",
    "            'deptName': parts[2].strip()\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def check_duplicate_comments(html_path):\n",
    "    \"\"\"HTML íŒŒì¼ì—ì„œ ë™ì¼ì¸ë¬¼ì˜ ì¤‘ë³µ ì˜ê²¬ í™•ì¸\"\"\"\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    \n",
    "    # user_spansì—ì„œ ì´ë¦„ ì¶”ì¶œ\n",
    "    names = []\n",
    "    user_spans = soup.find_all('span', class_='user')\n",
    "    \n",
    "    for user_span in user_spans:\n",
    "        name_elem = user_span.find('span', class_='F_12_black_b')\n",
    "        if name_elem:\n",
    "            info = extract_person_info(name_elem.get_text(strip=True))\n",
    "            if info:\n",
    "                names.append(info['name'])\n",
    "    \n",
    "    # ì¤‘ë³µ ì²´í¬\n",
    "    name_counts = Counter(names)\n",
    "    duplicates = {name: count for name, count in name_counts.items() if count > 1}\n",
    "    \n",
    "    return names, duplicates\n",
    "\n",
    "def scan_all_files(base_path):\n",
    "    \"\"\"ëª¨ë“  HTML íŒŒì¼ ìŠ¤ìº”\"\"\"\n",
    "    approval_path = Path(base_path) / 'ê²°ì¬'\n",
    "    \n",
    "    if not approval_path.exists():\n",
    "        print(f\"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {approval_path}\")\n",
    "        return\n",
    "    \n",
    "    html_files = list(approval_path.rglob('*.html'))\n",
    "    print(f\"ì´ {len(html_files)}ê°œì˜ HTML íŒŒì¼ ìŠ¤ìº” ì¤‘...\\n\")\n",
    "    \n",
    "    files_with_duplicates = []\n",
    "    total_duplicates = 0\n",
    "    \n",
    "    for idx, html_file in enumerate(html_files, 1):\n",
    "        try:\n",
    "            names, duplicates = check_duplicate_comments(html_file)\n",
    "            \n",
    "            if duplicates:\n",
    "                files_with_duplicates.append({\n",
    "                    'file': html_file.name,\n",
    "                    'path': str(html_file),\n",
    "                    'duplicates': duplicates,\n",
    "                    'all_names': names\n",
    "                })\n",
    "                total_duplicates += 1\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f\"ì§„í–‰: {idx}/{len(html_files)} (ì¤‘ë³µ ë°œê²¬: {total_duplicates}ê±´)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë¥˜ ({html_file.name}): {e}\")\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” ìŠ¤ìº” ì™„ë£Œ: {len(html_files)}ê°œ íŒŒì¼\")\n",
    "    print(f\"âš ï¸  ì¤‘ë³µ ë°œê²¬: {total_duplicates}ê°œ íŒŒì¼\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if files_with_duplicates:\n",
    "        print(\"ğŸ“‹ ì¤‘ë³µ ì˜ê²¬ì´ ìˆëŠ” íŒŒì¼ ëª©ë¡:\\n\")\n",
    "        for item in files_with_duplicates[:20]:  # ì²˜ìŒ 20ê°œë§Œ\n",
    "            print(f\"ğŸ“„ íŒŒì¼: {item['file']}\")\n",
    "            print(f\"   ê²½ë¡œ: {item['path']}\")\n",
    "            for name, count in item['duplicates'].items():\n",
    "                print(f\"   ğŸ‘¤ {name}: {count}ë²ˆ ë“±ì¥\")\n",
    "            print(f\"   ì „ì²´: {item['all_names']}\")\n",
    "            print()\n",
    "        \n",
    "        if len(files_with_duplicates) > 20:\n",
    "            print(f\"... ì™¸ {len(files_with_duplicates) - 20}ê°œ íŒŒì¼ ë” ìˆìŒ\\n\")\n",
    "    else:\n",
    "        print(\"âœ… ëª¨ë“  íŒŒì¼ì—ì„œ ì¤‘ë³µ ì—†ìŒ!\")\n",
    "    \n",
    "    return files_with_duplicates\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_path = r'C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html'\n",
    "    results = scan_all_files(base_path)\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´\n",
    "    if results:\n",
    "        with open('duplicate_comments.txt', 'w', encoding='utf-8') as f:\n",
    "            for item in results:\n",
    "                f.write(f\"íŒŒì¼: {item['file']}\\n\")\n",
    "                f.write(f\"ê²½ë¡œ: {item['path']}\\n\")\n",
    "                for name, count in item['duplicates'].items():\n",
    "                    f.write(f\"  - {name}: {count}ë²ˆ\\n\")\n",
    "                f.write(f\"  ì „ì²´: {item['all_names']}\\n\\n\")\n",
    "        print(\"ğŸ“ ê²°ê³¼ê°€ 'duplicate_comments.txt'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
