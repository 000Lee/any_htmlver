{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3efb80-6af1-42fb-bd54-f6e7f17f4dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\n",
      "\n",
      "âœ… ì´ 156ëª… ë¡œë“œ\n",
      "\n",
      "ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "\n",
      "âœ… 2002075: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002076: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002095: 'ê¹€íƒœì •'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002111: 'ê¹€íƒœì •'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002116: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002121: 'ê¹€ë¯¸í™”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002119: 'ê³ í™”ì„'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002125: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002146: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002165: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002164: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002168: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002174: 'ê³ í™”ì„'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002180: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002176: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002183: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002185: 'ê¹€ìˆ˜ì² '[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002189: 'ê³ í™”ì„'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002193: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002196: 'ê³ í™”ì„'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002199: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002202: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002204: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002209: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002208: 'ê¹€íƒœì •'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002206: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002212: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002215: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002216: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002220: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002225: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002222: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002228: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002227: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002243: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002256: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002252: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002264: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002267: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002266: 'ê¹€íƒœì •'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002271: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002276: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002279: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002280: 'ì´ë‚œê²½'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002292: 'ìœ¤ì§€í›ˆ'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002296: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002297: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002295: 'ê¹€ì„±ì—°'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002312: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002306: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002317: 'ê¹€í˜„ê¸¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002325: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002351: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002352: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002356: 'ê¶Œìˆœìˆ™'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002363: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002361: 'ê¹€ì„±ì—°'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002364: 'ìœ ì„ ì˜¤'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002367: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002366: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002381: 'ê¹€íƒœì •'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002386: 'ì¸í˜œë€'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002389: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002400: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002399: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002403: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002402: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002411: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002409: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002420: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002421: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002426: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002435: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002436: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002444: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002443: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002445: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002450: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002472: 'ìœ¤ì§€í›ˆ'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002476: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002486: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002509: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002510: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002508: 'ê¹€ëŒ€í™”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002507: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002506: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002523: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002532: 'ì´í˜•ê· '[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002531: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002539: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002542: 'ê¹€í˜„ê¸¸'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002543: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002545: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002554: 'ì´ì§„ê·¼'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002553: 'ì •í˜„ì •'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002558: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002559: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002565: 'ê¹€ì„±ê¶Œ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002566: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002583: 'ì •ìƒí˜¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002584: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002586: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002590: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002595: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002585: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002594: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002598: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002600: 'ê¹€í˜„ê¸¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002602: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002610: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002609: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002611: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002617: 'ê¹€ì„±ê¶Œ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002626: 'ì •ì£¼ì—°'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002631: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002629: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002630: 'ê¹€ë¯¸í™”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002634: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002637: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002641: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002642: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002643: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002650: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002649: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002651: 'ê¹€í™”ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002648: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002653: 'ì •ì£¼ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002654: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002657: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002665: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002661: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002662: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002660: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002664: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002663: 'ê¹€ì¬ì˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002666: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002670: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002673: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002671: 'ê¹€ì§„ì¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002676: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002674: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002675: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002677: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002682: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002684: 'ê¹€ê·œì¼'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002686: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002688: 'ê¶Œìˆœìˆ™'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002697: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002698: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002699: 'ê¹€í™”ì—°'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002704: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002707: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002701: 'ê¹€ëŒ€í™”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002703: 'ê¶Œìˆœìˆ™'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002710: 'ìœ¤ì§€í›ˆ'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002709: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002711: 'ê¹€í™”ì—°'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002712: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002715: 'ì •ì£¼ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002719: 'ì´í˜•ê· '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002720: 'ê¹€ë¯¸í™”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002718: 'ê¹€ê´‘í¬'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002724: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002726: 'ë§ˆìƒë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002727: 'ì´ì¢…ìˆ˜'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002729: 'ê¹€í™”ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002731: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002732: 'ì´í˜•ê· '[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002737: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002735: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002733: 'ìµœê¸°ì›'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002739: 'ê¹€í˜„ê¸¸'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002751: 'ê¹€ì§„ì¸'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002745: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002744: 'ë°•ì •ê· '[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002741: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002742: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002753: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002748: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002740: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002743: 'ì •ì£¼ì—°'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002754: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002762: 'ê¹€ëŒ€í™”'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002766: 'ê¹€ê¸°ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002770: 'ë°•ì •ê· '[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002773: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002772: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002780: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002776: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002779: 'ì›ì§„í¬'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2002778: 'ì›ì§„í¬'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2002782: 'ì´ì¢…ì›'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002793: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002791: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002790: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002789: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2002799: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002796: 'ì´ì¢…ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002798: 'ì›ì§„í¬'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2002802: 'ì •ì£¼ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002801: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002806: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002804: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002805: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002803: 'ì›ì§„í¬'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2002810: 'ê¹€ì§„ì¸'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002807: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002818: 'ê¹€ê¸°ë‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002812: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002813: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002814: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002819: 'ê¶Œìˆœìˆ™'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002827: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002821: 'ì´ë¯¼ê³¤'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002828: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002826: 'ì›ì§„í¬'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002829: 'ìµœê¸°ì›'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002833: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002834: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002839: 'í¸ì˜ìˆ˜'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2002841: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002837: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002842: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002844: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002848: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2002849: 'ì´ì¢…ìˆ˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002850: 'ë°°í˜„ì°¬'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002852: 'ê¹€ë¯¸í™”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002853: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002854: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002855: 'ì´ì„±ê¶Œ'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002856: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002857: 'ìœ¤ì§€í›ˆ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002858: 'ì •ì£¼ì—°'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002860: 'ì›ì§„í¬'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002862: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002863: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002866: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002868: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002870: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002865: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002867: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002873: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002875: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002877: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002874: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002878: 'ê¹€ì„±ì—°'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002879: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002881: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002886: 'ê¹€í™”ì—°'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002883: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002882: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002889: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002896: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2002893: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002895: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002891: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2002898: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2002892: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2002899: 'ìµœê¸°ì›'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002900: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002903: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002902: 'ë°•ì •ê· '[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2002901: 'í¸ì˜ìˆ˜'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2002906: 'ê¹€ì€ì˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "\n",
      "ğŸ• í¬ë¡¤ë§ í•„ìš”í•œ ë¬¸ì„œ: 265ê±´\n",
      "   ì €ì¥ ìœ„ì¹˜: need_crawling.json\n",
      "\n",
      "=== ì²˜ë¦¬ ì™„ë£Œ ===\n",
      "ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: 265ê±´\n",
      "ìˆ˜ì • ì„±ê³µ: 265ê±´\n"
     ]
    }
   ],
   "source": [
    "# drafter ì´ë¦„ ìˆëŠ”ë° ê²°ì¬ì„  ì—†ëŠ”ê²ƒë„ ì²˜ë¦¬ í•  ìˆ˜ ìˆê²Œ ì½”ë“œ ìˆ˜ì •\n",
    "\n",
    "# drafter, ê²°ì¬ì„  ì´ë¦„, ì¡°ì§ë„ë°˜ì˜. ì—†ëŠ” ê²ƒë“¤ì˜ ì‹œê°„ì€ 00 00 00 ìœ¼ë¡œ\n",
    "\n",
    "# ì¡°ì§ë„ë„ ë°˜ì˜ + actionDate 00:00:00ì¸ sourceId ì €ì¥\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ì„¤ì •\n",
    "cmds_file = 'documents_2010.cmds'\n",
    "output_file = 'documents_2010_fixed.cmds'\n",
    "html_base_path = r'C:\\Users\\LEEJUHWAN\\Downloads\\2010-01-01~2010-12-31\\html\\ê²°ì¬'\n",
    "CSV_FILE = 'ì¸ì‚¬ì •ë³´_ë¶€ì„œì½”ë“œì¶”ê°€.csv'\n",
    "NEED_CRAWLING_FILE = 'need_crawling.json'  # â† ì¶”ê°€!\n",
    "\n",
    "# CSV ì½ê¸°\n",
    "print(\"ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\\n\")\n",
    "df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')\n",
    "\n",
    "employee_dict = {}\n",
    "for _, row in df.iterrows():\n",
    "    employee_dict[row['ì‚¬ì›ëª…']] = {\n",
    "        'emailId': row['ID'],\n",
    "        'deptName': row['ë¶€ì„œ'],\n",
    "        'empNo': row['ì‚¬ì›ë²ˆí˜¸'] if pd.notna(row['ì‚¬ì›ë²ˆí˜¸']) else '',\n",
    "        'positionName': row['ì§ìœ„'] if pd.notna(row['ì§ìœ„']) else '',\n",
    "        'deptCode': row['ë¶€ì„œì½”ë“œ'] if pd.notna(row['ë¶€ì„œì½”ë“œ']) else ''\n",
    "    }\n",
    "print(f\"âœ… ì´ {len(employee_dict)}ëª… ë¡œë“œ\\n\")\n",
    "\n",
    "def find_html_file(source_id, base_path):\n",
    "    \"\"\"sourceIdë¡œ HTML íŒŒì¼ ì°¾ê¸°\"\"\"\n",
    "    for html_file in Path(base_path).rglob('*.html'):\n",
    "        if source_id in html_file.stem:\n",
    "            return html_file\n",
    "    return None\n",
    "\n",
    "def extract_drafter_and_activities(html_path):\n",
    "    \"\"\"HTMLì—ì„œ ê¸°ì•ˆì, activities, createdAt ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "        # 1. ê¸°ì•ˆì ì´ë¦„\n",
    "        drafter_name = ''\n",
    "        drafter_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì' in s)\n",
    "        if drafter_th:\n",
    "            drafter_td = drafter_th.find_next_sibling('td')\n",
    "            if drafter_td:\n",
    "                bg01_div = drafter_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    drafter_name = bg01_div.get_text(strip=True)\n",
    "        \n",
    "        # 2. ê¸°ì•ˆì¼\n",
    "        created_at = None\n",
    "        created_year = None\n",
    "        created_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì¼' in s)\n",
    "        if created_th:\n",
    "            created_td = created_th.find_next_sibling('td')\n",
    "            if created_td:\n",
    "                bg01_div = created_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    date_str = bg01_div.get_text(strip=True)\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        created_at = int(dt.timestamp() * 1000)\n",
    "                        created_year = dt.year\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 3. activities\n",
    "        activities = []\n",
    "        needs_crawling = []  # â† ì¶”ê°€! í¬ë¡¤ë§ í•„ìš”í•œ í•­ëª©\n",
    "        bg02_divs = soup.find_all('div', class_='bg02')\n",
    "        \n",
    "        for idx, bg02 in enumerate(bg02_divs):\n",
    "            ul = bg02.find('ul')\n",
    "            if ul:\n",
    "                lis = ul.find_all('li')\n",
    "                if len(lis) >= 2:\n",
    "                    name = lis[0].get_text(strip=True)\n",
    "                    action_type_text = lis[1].get_text(strip=True)\n",
    "                    date_text = lis[2].get_text(strip=True) if len(lis) >= 3 else ''\n",
    "                    \n",
    "                    if name:\n",
    "                        if 'ê¸°ì•ˆ' in action_type_text:\n",
    "                            action_type = 'DRAFT'\n",
    "                        elif 'í•©ì˜' in action_type_text:\n",
    "                            action_type = 'AGREEMENT'\n",
    "                        else:\n",
    "                            action_type = 'APPROVAL'\n",
    "                        \n",
    "                        # actionDate\n",
    "                        action_date = None\n",
    "                        if date_text and created_year:\n",
    "                            try:\n",
    "                                full_date = f\"{created_year}-{date_text.replace('/', '-')}\"\n",
    "                                dt = datetime.strptime(full_date, \"%Y-%m-%d\")\n",
    "                                action_date = int(dt.timestamp() * 1000)\n",
    "                                \n",
    "                                # ì‹œê°„ì´ 00:00:00ì´ë©´ í¬ë¡¤ë§ í•„ìš” â† ì¶”ê°€!\n",
    "                                needs_crawling.append({\n",
    "                                    'index': idx,\n",
    "                                    'name': name,\n",
    "                                    'date_text': date_text,\n",
    "                                    'action_type': action_type\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            position = employee_dict[name]['positionName']\n",
    "                            dept = employee_dict[name]['deptName']\n",
    "                            email = employee_dict[name]['emailId']\n",
    "                            dept_code = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            position = ''\n",
    "                            dept = ''\n",
    "                            email = ''\n",
    "                            dept_code = ''\n",
    "                        \n",
    "                        activities.append({\n",
    "                            'positionName': position,\n",
    "                            'deptName': dept,\n",
    "                            'actionLogType': action_type,\n",
    "                            'name': name,\n",
    "                            'emailId': email,\n",
    "                            'type': action_type,\n",
    "                            'actionDate': action_date,\n",
    "                            'deptCode': dept_code,\n",
    "                            'actionComment': ''\n",
    "                        })\n",
    "        \n",
    "        return drafter_name, activities, created_at, needs_crawling  # â† needs_crawling ì¶”ê°€!\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ HTML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        return '', [], None, []\n",
    "\n",
    "# cmds íŒŒì¼ ì²˜ë¦¬\n",
    "print(\"ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\\n\")\n",
    "\n",
    "crawling_needed = {}  # â† ì¶”ê°€! sourceIdë³„ í¬ë¡¤ë§ í•„ìš” ì •ë³´\n",
    "\n",
    "with open(cmds_file, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    \n",
    "    fixed_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for line in f_in:\n",
    "        if line.startswith('addDocument '):\n",
    "            json_str = line[12:].strip()\n",
    "            doc = json.loads(json_str)\n",
    "            \n",
    "            drafter_name = doc.get('drafter', {}).get('name', '').strip()\n",
    "            activities = doc.get('activities', [])\n",
    "            if not drafter_name or not activities:\n",
    "                total_count += 1\n",
    "                source_id = doc.get('sourceId')\n",
    "                \n",
    "                html_file = find_html_file(source_id, html_base_path)\n",
    "                \n",
    "                if html_file:\n",
    "                    name, activities, created_at, needs_crawling = extract_drafter_and_activities(html_file)\n",
    "                    \n",
    "                    if name:\n",
    "                        # drafterì— ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = employee_dict[name]['positionName']\n",
    "                            doc['drafter']['deptName'] = employee_dict[name]['deptName']\n",
    "                            doc['drafter']['emailId'] = employee_dict[name]['emailId']\n",
    "                            doc['drafter']['deptCode'] = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = ''\n",
    "                            doc['drafter']['deptName'] = ''\n",
    "                            doc['drafter']['emailId'] = 'master'\n",
    "                            doc['drafter']['deptCode'] = ''\n",
    "                        \n",
    "                        doc['activities'] = activities\n",
    "                        doc['createdAt'] = created_at\n",
    "                        fixed_count += 1\n",
    "                        \n",
    "                        # í¬ë¡¤ë§ í•„ìš”í•œ ê²½ìš° ì €ì¥ â† ì¶”ê°€!\n",
    "                        if needs_crawling:\n",
    "                            crawling_needed[source_id] = needs_crawling\n",
    "                        \n",
    "                        status = \"í˜„ì§\" if name in employee_dict else \"í‡´ì‚¬\"\n",
    "                        crawl_mark = \" ğŸ•\" if needs_crawling else \"\"\n",
    "                        print(f\"âœ… {source_id}: '{name}'[{status}] + {len(activities)}ê°œ{crawl_mark}\")\n",
    "            \n",
    "            f_out.write(f\"addDocument {json.dumps(doc, ensure_ascii=False, separators=(',', ':'))}\\n\")\n",
    "        else:\n",
    "            f_out.write(line)\n",
    "\n",
    "# í¬ë¡¤ë§ í•„ìš” ì •ë³´ ì €ì¥ â† ì¶”ê°€!\n",
    "if crawling_needed:\n",
    "    with open(NEED_CRAWLING_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(crawling_needed, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ• í¬ë¡¤ë§ í•„ìš”í•œ ë¬¸ì„œ: {len(crawling_needed)}ê±´\")\n",
    "    print(f\"   ì €ì¥ ìœ„ì¹˜: {NEED_CRAWLING_FILE}\")\n",
    "\n",
    "print(f\"\\n=== ì²˜ë¦¬ ì™„ë£Œ ===\")\n",
    "print(f\"ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: {total_count}ê±´\")\n",
    "print(f\"ìˆ˜ì • ì„±ê³µ: {fixed_count}ê±´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc2a63b-224b-4b66-92ad-0227756452df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\n",
      "\n",
      "âœ… ì´ 156ëª… ë¡œë“œ\n",
      "\n",
      "ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "\n",
      "\n",
      "=== ì²˜ë¦¬ ì™„ë£Œ ===\n",
      "ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: 0ê±´\n",
      "ìˆ˜ì • ì„±ê³µ: 0ê±´\n"
     ]
    }
   ],
   "source": [
    "# drafter, ê²°ì¬ì„  ì´ë¦„, ì¡°ì§ë„ë°˜ì˜. ì—†ëŠ” ê²ƒë“¤ì˜ ì‹œê°„ì€ 00 00 00 ìœ¼ë¡œ\n",
    "\n",
    "# ì¡°ì§ë„ë„ ë°˜ì˜ + actionDate 00:00:00ì¸ sourceId ì €ì¥\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ì„¤ì •\n",
    "cmds_file = 'documents_2015.cmds'\n",
    "output_file = 'documents_2015_fixed.cmds'\n",
    "html_base_path = r'C:\\Users\\LEEJUHWAN\\Downloads\\2011-01-01~2015-12-31\\html\\ê²°ì¬'\n",
    "CSV_FILE = 'ì¸ì‚¬ì •ë³´_ë¶€ì„œì½”ë“œì¶”ê°€.csv'\n",
    "NEED_CRAWLING_FILE = 'need_crawling.json'  # â† ì¶”ê°€!\n",
    "\n",
    "# CSV ì½ê¸°\n",
    "print(\"ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\\n\")\n",
    "df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')\n",
    "\n",
    "employee_dict = {}\n",
    "for _, row in df.iterrows():\n",
    "    employee_dict[row['ì‚¬ì›ëª…']] = {\n",
    "        'emailId': row['ID'],\n",
    "        'deptName': row['ë¶€ì„œ'],\n",
    "        'empNo': row['ì‚¬ì›ë²ˆí˜¸'] if pd.notna(row['ì‚¬ì›ë²ˆí˜¸']) else '',\n",
    "        'positionName': row['ì§ìœ„'] if pd.notna(row['ì§ìœ„']) else '',\n",
    "        'deptCode': row['ë¶€ì„œì½”ë“œ'] if pd.notna(row['ë¶€ì„œì½”ë“œ']) else ''\n",
    "    }\n",
    "print(f\"âœ… ì´ {len(employee_dict)}ëª… ë¡œë“œ\\n\")\n",
    "\n",
    "def find_html_file(source_id, base_path):\n",
    "    \"\"\"sourceIdë¡œ HTML íŒŒì¼ ì°¾ê¸°\"\"\"\n",
    "    for html_file in Path(base_path).rglob('*.html'):\n",
    "        if source_id in html_file.stem:\n",
    "            return html_file\n",
    "    return None\n",
    "\n",
    "def extract_drafter_and_activities(html_path):\n",
    "    \"\"\"HTMLì—ì„œ ê¸°ì•ˆì, activities, createdAt ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "        # 1. ê¸°ì•ˆì ì´ë¦„\n",
    "        drafter_name = ''\n",
    "        drafter_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì' in s)\n",
    "        if drafter_th:\n",
    "            drafter_td = drafter_th.find_next_sibling('td')\n",
    "            if drafter_td:\n",
    "                bg01_div = drafter_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    drafter_name = bg01_div.get_text(strip=True)\n",
    "        \n",
    "        # 2. ê¸°ì•ˆì¼\n",
    "        created_at = None\n",
    "        created_year = None\n",
    "        created_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì¼' in s)\n",
    "        if created_th:\n",
    "            created_td = created_th.find_next_sibling('td')\n",
    "            if created_td:\n",
    "                bg01_div = created_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    date_str = bg01_div.get_text(strip=True)\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        created_at = int(dt.timestamp() * 1000)\n",
    "                        created_year = dt.year\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 3. activities\n",
    "        activities = []\n",
    "        needs_crawling = []  # â† ì¶”ê°€! í¬ë¡¤ë§ í•„ìš”í•œ í•­ëª©\n",
    "        bg02_divs = soup.find_all('div', class_='bg02')\n",
    "        \n",
    "        for idx, bg02 in enumerate(bg02_divs):\n",
    "            ul = bg02.find('ul')\n",
    "            if ul:\n",
    "                lis = ul.find_all('li')\n",
    "                if len(lis) >= 2:\n",
    "                    name = lis[0].get_text(strip=True)\n",
    "                    action_type_text = lis[1].get_text(strip=True)\n",
    "                    date_text = lis[2].get_text(strip=True) if len(lis) >= 3 else ''\n",
    "                    \n",
    "                    if name:\n",
    "                        if 'ê¸°ì•ˆ' in action_type_text:\n",
    "                            action_type = 'DRAFT'\n",
    "                        elif 'í•©ì˜' in action_type_text:\n",
    "                            action_type = 'AGREEMENT'\n",
    "                        else:\n",
    "                            action_type = 'APPROVAL'\n",
    "                        \n",
    "                        # actionDate\n",
    "                        action_date = None\n",
    "                        if date_text and created_year:\n",
    "                            try:\n",
    "                                full_date = f\"{created_year}-{date_text.replace('/', '-')}\"\n",
    "                                dt = datetime.strptime(full_date, \"%Y-%m-%d\")\n",
    "                                action_date = int(dt.timestamp() * 1000)\n",
    "                                \n",
    "                                # ì‹œê°„ì´ 00:00:00ì´ë©´ í¬ë¡¤ë§ í•„ìš” â† ì¶”ê°€!\n",
    "                                needs_crawling.append({\n",
    "                                    'index': idx,\n",
    "                                    'name': name,\n",
    "                                    'date_text': date_text,\n",
    "                                    'action_type': action_type\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            position = employee_dict[name]['positionName']\n",
    "                            dept = employee_dict[name]['deptName']\n",
    "                            email = employee_dict[name]['emailId']\n",
    "                            dept_code = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            position = ''\n",
    "                            dept = ''\n",
    "                            email = ''\n",
    "                            dept_code = ''\n",
    "                        \n",
    "                        activities.append({\n",
    "                            'positionName': position,\n",
    "                            'deptName': dept,\n",
    "                            'actionLogType': action_type,\n",
    "                            'name': name,\n",
    "                            'emailId': email,\n",
    "                            'type': action_type,\n",
    "                            'actionDate': action_date,\n",
    "                            'deptCode': dept_code,\n",
    "                            'actionComment': ''\n",
    "                        })\n",
    "        \n",
    "        return drafter_name, activities, created_at, needs_crawling  # â† needs_crawling ì¶”ê°€!\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ HTML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        return '', [], None, []\n",
    "\n",
    "# cmds íŒŒì¼ ì²˜ë¦¬\n",
    "print(\"ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\\n\")\n",
    "\n",
    "crawling_needed = {}  # â† ì¶”ê°€! sourceIdë³„ í¬ë¡¤ë§ í•„ìš” ì •ë³´\n",
    "\n",
    "with open(cmds_file, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    \n",
    "    fixed_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for line in f_in:\n",
    "        if line.startswith('addDocument '):\n",
    "            json_str = line[12:].strip()\n",
    "            doc = json.loads(json_str)\n",
    "            \n",
    "            drafter_name = doc.get('drafter', {}).get('name', '').strip()\n",
    "            if not drafter_name:\n",
    "                total_count += 1\n",
    "                source_id = doc.get('sourceId')\n",
    "                \n",
    "                html_file = find_html_file(source_id, html_base_path)\n",
    "                \n",
    "                if html_file:\n",
    "                    name, activities, created_at, needs_crawling = extract_drafter_and_activities(html_file)\n",
    "                    \n",
    "                    if name:\n",
    "                        # drafterì— ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = employee_dict[name]['positionName']\n",
    "                            doc['drafter']['deptName'] = employee_dict[name]['deptName']\n",
    "                            doc['drafter']['emailId'] = employee_dict[name]['emailId']\n",
    "                            doc['drafter']['deptCode'] = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = ''\n",
    "                            doc['drafter']['deptName'] = ''\n",
    "                            doc['drafter']['emailId'] = 'master'\n",
    "                            doc['drafter']['deptCode'] = ''\n",
    "                        \n",
    "                        doc['activities'] = activities\n",
    "                        doc['createdAt'] = created_at\n",
    "                        fixed_count += 1\n",
    "                        \n",
    "                        # í¬ë¡¤ë§ í•„ìš”í•œ ê²½ìš° ì €ì¥ â† ì¶”ê°€!\n",
    "                        if needs_crawling:\n",
    "                            crawling_needed[source_id] = needs_crawling\n",
    "                        \n",
    "                        status = \"í˜„ì§\" if name in employee_dict else \"í‡´ì‚¬\"\n",
    "                        crawl_mark = \" ğŸ•\" if needs_crawling else \"\"\n",
    "                        print(f\"âœ… {source_id}: '{name}'[{status}] + {len(activities)}ê°œ{crawl_mark}\")\n",
    "            \n",
    "            f_out.write(f\"addDocument {json.dumps(doc, ensure_ascii=False, separators=(',', ':'))}\\n\")\n",
    "        else:\n",
    "            f_out.write(line)\n",
    "\n",
    "# í¬ë¡¤ë§ í•„ìš” ì •ë³´ ì €ì¥ â† ì¶”ê°€!\n",
    "if crawling_needed:\n",
    "    with open(NEED_CRAWLING_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(crawling_needed, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ• í¬ë¡¤ë§ í•„ìš”í•œ ë¬¸ì„œ: {len(crawling_needed)}ê±´\")\n",
    "    print(f\"   ì €ì¥ ìœ„ì¹˜: {NEED_CRAWLING_FILE}\")\n",
    "\n",
    "print(f\"\\n=== ì²˜ë¦¬ ì™„ë£Œ ===\")\n",
    "print(f\"ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: {total_count}ê±´\")\n",
    "print(f\"ìˆ˜ì • ì„±ê³µ: {fixed_count}ê±´\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
