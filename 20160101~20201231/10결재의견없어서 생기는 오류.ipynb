{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc2a63b-224b-4b66-92ad-0227756452df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\n",
      "\n",
      "âœ… ì´ 156ëª… ë¡œë“œ\n",
      "\n",
      "ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "\n",
      "âœ… 2009497: 'ê¹€ì€ë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009500: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009499: 'ê¹€ì€ë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009498: 'ê¹€ì€ë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009507: 'ì›ì§„í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009508: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009506: 'í¸ì˜ìˆ˜'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009509: 'ê¹€ìŠ¹ë²”'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009510: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009511: 'ì´í˜•ê· '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009512: 'ì „ìˆ˜ì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009513: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009514: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009515: 'ì´ì†¡ì—°'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009519: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009518: 'ê°•ì •ë¡€'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009517: 'ê°•ì •ë¡€'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009523: 'ê¹€í˜•í™˜'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2009524: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009525: 'ê°•ì •ë¡€'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009526: 'ì¥ê´‘ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009536: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009531: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009530: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009532: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009533: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009527: 'ì„í˜„ì¤€'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2009529: 'ì›ì§„í¬'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009535: 'ê°•ì •ë¡€'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2009528: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009538: 'ì´ì„±í˜„'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009544: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009540: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009543: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009542: 'ê¹€ê´‘í¬'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009545: 'ì†Œì„±í˜„'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009547: 'ìœ ì„±ìˆ™'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009548: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009550: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009552: 'í¸ì˜ìˆ˜'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2009553: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009551: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009549: 'ì„í˜„ì¤€'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009557: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009556: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009562: 'ì •ì£¼ì—°'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009560: 'ì •ì£¼ì—°'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009563: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009559: 'ì „ìˆ˜ì •'[í‡´ì‚¬] + 6ê°œ ğŸ•\n",
      "âœ… 2009565: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009566: 'ê¹€ì„±ìˆ˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009567: 'ì„í˜„ì¤€'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009569: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009570: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009574: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009573: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009575: 'ì´í˜•ê· '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009571: 'ë°•ìƒê¶Œ'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009572: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009577: 'ì•ˆì†Œí˜„'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009580: 'ë¥˜ê´‘ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009579: 'í•¨ì§€ì—°'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009587: 'ì›ì§„í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009586: 'ê¹€í•˜ì‘'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009585: 'ë¬¸ê·œì›'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009584: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009590: 'ê¹€ìŠ¹ë²”'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009592: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009598: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009594: 'ìœ ë¯¼ê·œ'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009597: 'ì›ì§„í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009599: 'ì´ë‘ì§„'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009600: 'ê¹€í˜•í™˜'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009602: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009601: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009606: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009607: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009605: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009612: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009611: 'í•œì •ìœ¤'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009620: 'ì´ë™í•˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009624: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009623: 'ì†Œì„±í˜„'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009625: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009627: 'ì´ë™í•˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009629: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009628: 'ë°°ì„¤í¬'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009631: 'ë°•í˜•ì„'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009630: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009634: 'ì¥ê´‘ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009633: 'ë°•ì •ê· '[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009635: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009632: 'ì´ì„±í˜„'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009638: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009637: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009643: 'ì •ì£¼ì—°'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009641: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009642: 'ì†¡ë³‘í˜¸'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009648: 'ì¥ì„±ë´‰'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009650: 'ì´ì„±í˜„'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009651: 'í•˜í˜„ìš°'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009645: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009657: 'ì¥ê´‘ì„ '[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009659: 'ì´ì„¸ì›'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009658: 'ì‹ ì„±í˜¸'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2009656: 'ì†í•œì„'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009652: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009661: 'ì›ì§„í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009663: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009665: 'ê¹€ë™ë ¥'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009668: 'ê¹€í˜•ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009664: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009662: 'ì›ì§„í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009667: 'ì–‘ê¸°í›ˆ'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009670: 'ì •ê²½ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009666: 'ì´ë™í•˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009673: 'ìœ ì„±ìˆ™'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009681: 'ê¹€ê´‘í¬'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009675: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009685: 'ì´í˜•ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009684: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009680: 'ê¹€íƒœì •'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009677: 'ê¹€ì¬ì›'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009686: 'ì§„ë¯¼ê· '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009694: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009695: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009689: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009696: 'ë°•í¬ë¹ˆ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009692: 'ë°°ì„¤í¬'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009693: 'ê¹€í˜„ìˆ˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009691: 'ë°•ë¯¼ì˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009688: 'ì´ë™í•˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009687: 'ë°•ë¯¼ì˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009705: 'ë§ˆìƒë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009697: 'ê¹€í˜•ì² '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009703: 'ì¥ê´‘ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009704: 'ì´ì†¡ì—°'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009707: 'í¸ì˜ìˆ˜'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009708: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009712: 'ê¹€ë¬¸ì² '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009721: 'ì´ë™í•˜'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009722: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009714: 'ì´ë™í•˜'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009716: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009718: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009723: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009715: 'ê¹€ì¬ì›'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009717: 'ì´ìŠ¹ë¯¼'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009720: 'ì–‘íƒœì›…'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009724: 'ê¹€íƒœì •'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009726: 'ê¹€í˜„ìˆ˜'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009728: 'ê¹€ì€ì§„'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009732: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009733: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009734: 'ê¹€ì§€ì›'[í‡´ì‚¬] + 5ê°œ ğŸ•\n",
      "âœ… 2009735: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009737: 'í•¨ì¢…ì™„'[í‡´ì‚¬] + 4ê°œ ğŸ•\n",
      "âœ… 2009739: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009738: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009741: 'ìœ ì„±ìˆ™'[í˜„ì§] + 5ê°œ ğŸ•\n",
      "âœ… 2009742: 'ì„ì„±ì§„'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009748: 'ë°•í¬ë¹ˆ'[í˜„ì§] + 4ê°œ ğŸ•\n",
      "âœ… 2009746: 'ì†Œì„±í˜„'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009752: 'ê¶Œë¯¼ì„ '[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "âœ… 2009755: 'í¸ì˜ìˆ˜'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009758: 'ê¹€ë¯¼ì„œ'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009757: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 2ê°œ ğŸ•\n",
      "âœ… 2009754: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009760: 'í˜„ê´‘ì„­'[í‡´ì‚¬] + 2ê°œ ğŸ•\n",
      "âœ… 2009756: 'ìœ¤ìƒí˜¸'[í˜„ì§] + 3ê°œ ğŸ•\n",
      "âœ… 2009762: 'ë§ˆìƒë¯¸'[í‡´ì‚¬] + 3ê°œ ğŸ•\n",
      "\n",
      "ğŸ• í¬ë¡¤ë§ í•„ìš”í•œ ë¬¸ì„œ: 171ê±´\n",
      "   ì €ì¥ ìœ„ì¹˜: need_crawling.json\n",
      "\n",
      "=== ì²˜ë¦¬ ì™„ë£Œ ===\n",
      "ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: 171ê±´\n",
      "ìˆ˜ì • ì„±ê³µ: 171ê±´\n"
     ]
    }
   ],
   "source": [
    "# drafter, ê²°ì¬ì„  ì´ë¦„, ì¡°ì§ë„ë°˜ì˜. ì—†ëŠ” ê²ƒë“¤ì˜ ì‹œê°„ì€ 00 00 00 ìœ¼ë¡œ\n",
    "\n",
    "# ì¡°ì§ë„ë„ ë°˜ì˜ + actionDate 00:00:00ì¸ sourceId ì €ì¥\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ì„¤ì •\n",
    "cmds_file = 'documents_2020.cmds'\n",
    "output_file = 'documents_2020_fixed.cmds'\n",
    "html_base_path = r'C:\\Users\\LEEJUHWAN\\Downloads\\2016-01-01~2020-12-31\\html\\ê²°ì¬'\n",
    "CSV_FILE = 'ì¸ì‚¬ì •ë³´_ë¶€ì„œì½”ë“œì¶”ê°€.csv'\n",
    "NEED_CRAWLING_FILE = 'need_crawling.json'  # â† ì¶”ê°€!\n",
    "\n",
    "# CSV ì½ê¸°\n",
    "print(\"ğŸ“Š CSV ì¡°ì§ë„ ë¡œë“œ ì¤‘...\\n\")\n",
    "df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')\n",
    "\n",
    "employee_dict = {}\n",
    "for _, row in df.iterrows():\n",
    "    employee_dict[row['ì‚¬ì›ëª…']] = {\n",
    "        'emailId': row['ID'],\n",
    "        'deptName': row['ë¶€ì„œ'],\n",
    "        'empNo': row['ì‚¬ì›ë²ˆí˜¸'] if pd.notna(row['ì‚¬ì›ë²ˆí˜¸']) else '',\n",
    "        'positionName': row['ì§ìœ„'] if pd.notna(row['ì§ìœ„']) else '',\n",
    "        'deptCode': row['ë¶€ì„œì½”ë“œ'] if pd.notna(row['ë¶€ì„œì½”ë“œ']) else ''\n",
    "    }\n",
    "print(f\"âœ… ì´ {len(employee_dict)}ëª… ë¡œë“œ\\n\")\n",
    "\n",
    "def find_html_file(source_id, base_path):\n",
    "    \"\"\"sourceIdë¡œ HTML íŒŒì¼ ì°¾ê¸°\"\"\"\n",
    "    for html_file in Path(base_path).rglob('*.html'):\n",
    "        if source_id in html_file.stem:\n",
    "            return html_file\n",
    "    return None\n",
    "\n",
    "def extract_drafter_and_activities(html_path):\n",
    "    \"\"\"HTMLì—ì„œ ê¸°ì•ˆì, activities, createdAt ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "        # 1. ê¸°ì•ˆì ì´ë¦„\n",
    "        drafter_name = ''\n",
    "        drafter_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì' in s)\n",
    "        if drafter_th:\n",
    "            drafter_td = drafter_th.find_next_sibling('td')\n",
    "            if drafter_td:\n",
    "                bg01_div = drafter_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    drafter_name = bg01_div.get_text(strip=True)\n",
    "        \n",
    "        # 2. ê¸°ì•ˆì¼\n",
    "        created_at = None\n",
    "        created_year = None\n",
    "        created_th = soup.find('th', string=lambda s: s and 'ê¸°ì•ˆì¼' in s)\n",
    "        if created_th:\n",
    "            created_td = created_th.find_next_sibling('td')\n",
    "            if created_td:\n",
    "                bg01_div = created_td.find('div', class_='bg01')\n",
    "                if bg01_div:\n",
    "                    date_str = bg01_div.get_text(strip=True)\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        created_at = int(dt.timestamp() * 1000)\n",
    "                        created_year = dt.year\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 3. activities\n",
    "        activities = []\n",
    "        needs_crawling = []  # â† ì¶”ê°€! í¬ë¡¤ë§ í•„ìš”í•œ í•­ëª©\n",
    "        bg02_divs = soup.find_all('div', class_='bg02')\n",
    "        \n",
    "        for idx, bg02 in enumerate(bg02_divs):\n",
    "            ul = bg02.find('ul')\n",
    "            if ul:\n",
    "                lis = ul.find_all('li')\n",
    "                if len(lis) >= 2:\n",
    "                    name = lis[0].get_text(strip=True)\n",
    "                    action_type_text = lis[1].get_text(strip=True)\n",
    "                    date_text = lis[2].get_text(strip=True) if len(lis) >= 3 else ''\n",
    "                    \n",
    "                    if name:\n",
    "                        if 'ê¸°ì•ˆ' in action_type_text:\n",
    "                            action_type = 'DRAFT'\n",
    "                        elif 'í•©ì˜' in action_type_text:\n",
    "                            action_type = 'AGREEMENT'\n",
    "                        else:\n",
    "                            action_type = 'APPROVAL'\n",
    "                        \n",
    "                        # actionDate\n",
    "                        action_date = None\n",
    "                        if date_text and created_year:\n",
    "                            try:\n",
    "                                full_date = f\"{created_year}-{date_text.replace('/', '-')}\"\n",
    "                                dt = datetime.strptime(full_date, \"%Y-%m-%d\")\n",
    "                                action_date = int(dt.timestamp() * 1000)\n",
    "                                \n",
    "                                # ì‹œê°„ì´ 00:00:00ì´ë©´ í¬ë¡¤ë§ í•„ìš” â† ì¶”ê°€!\n",
    "                                needs_crawling.append({\n",
    "                                    'index': idx,\n",
    "                                    'name': name,\n",
    "                                    'date_text': date_text,\n",
    "                                    'action_type': action_type\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            position = employee_dict[name]['positionName']\n",
    "                            dept = employee_dict[name]['deptName']\n",
    "                            email = employee_dict[name]['emailId']\n",
    "                            dept_code = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            position = ''\n",
    "                            dept = ''\n",
    "                            email = ''\n",
    "                            dept_code = ''\n",
    "                        \n",
    "                        activities.append({\n",
    "                            'positionName': position,\n",
    "                            'deptName': dept,\n",
    "                            'actionLogType': action_type,\n",
    "                            'name': name,\n",
    "                            'emailId': email,\n",
    "                            'type': action_type,\n",
    "                            'actionDate': action_date,\n",
    "                            'deptCode': dept_code,\n",
    "                            'actionComment': ''\n",
    "                        })\n",
    "        \n",
    "        return drafter_name, activities, created_at, needs_crawling  # â† needs_crawling ì¶”ê°€!\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ HTML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "        return '', [], None, []\n",
    "\n",
    "# cmds íŒŒì¼ ì²˜ë¦¬\n",
    "print(\"ğŸ“„ cmds íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\\n\")\n",
    "\n",
    "crawling_needed = {}  # â† ì¶”ê°€! sourceIdë³„ í¬ë¡¤ë§ í•„ìš” ì •ë³´\n",
    "\n",
    "with open(cmds_file, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    \n",
    "    fixed_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for line in f_in:\n",
    "        if line.startswith('addDocument '):\n",
    "            json_str = line[12:].strip()\n",
    "            doc = json.loads(json_str)\n",
    "            \n",
    "            drafter_name = doc.get('drafter', {}).get('name', '').strip()\n",
    "            if not drafter_name:\n",
    "                total_count += 1\n",
    "                source_id = doc.get('sourceId')\n",
    "                \n",
    "                html_file = find_html_file(source_id, html_base_path)\n",
    "                \n",
    "                if html_file:\n",
    "                    name, activities, created_at, needs_crawling = extract_drafter_and_activities(html_file)\n",
    "                    \n",
    "                    if name:\n",
    "                        # drafterì— ì¡°ì§ë„ ì ìš©\n",
    "                        if name in employee_dict:\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = employee_dict[name]['positionName']\n",
    "                            doc['drafter']['deptName'] = employee_dict[name]['deptName']\n",
    "                            doc['drafter']['emailId'] = employee_dict[name]['emailId']\n",
    "                            doc['drafter']['deptCode'] = employee_dict[name]['deptCode']\n",
    "                        else:\n",
    "                            # í‡´ì‚¬ì\n",
    "                            doc['drafter']['name'] = name\n",
    "                            doc['drafter']['positionName'] = ''\n",
    "                            doc['drafter']['deptName'] = ''\n",
    "                            doc['drafter']['emailId'] = 'master'\n",
    "                            doc['drafter']['deptCode'] = ''\n",
    "                        \n",
    "                        doc['activities'] = activities\n",
    "                        doc['createdAt'] = created_at\n",
    "                        fixed_count += 1\n",
    "                        \n",
    "                        # í¬ë¡¤ë§ í•„ìš”í•œ ê²½ìš° ì €ì¥ â† ì¶”ê°€!\n",
    "                        if needs_crawling:\n",
    "                            crawling_needed[source_id] = needs_crawling\n",
    "                        \n",
    "                        status = \"í˜„ì§\" if name in employee_dict else \"í‡´ì‚¬\"\n",
    "                        crawl_mark = \" ğŸ•\" if needs_crawling else \"\"\n",
    "                        print(f\"âœ… {source_id}: '{name}'[{status}] + {len(activities)}ê°œ{crawl_mark}\")\n",
    "            \n",
    "            f_out.write(f\"addDocument {json.dumps(doc, ensure_ascii=False, separators=(',', ':'))}\\n\")\n",
    "        else:\n",
    "            f_out.write(line)\n",
    "\n",
    "# í¬ë¡¤ë§ í•„ìš” ì •ë³´ ì €ì¥ â† ì¶”ê°€!\n",
    "if crawling_needed:\n",
    "    with open(NEED_CRAWLING_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(crawling_needed, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ• í¬ë¡¤ë§ í•„ìš”í•œ ë¬¸ì„œ: {len(crawling_needed)}ê±´\")\n",
    "    print(f\"   ì €ì¥ ìœ„ì¹˜: {NEED_CRAWLING_FILE}\")\n",
    "\n",
    "print(f\"\\n=== ì²˜ë¦¬ ì™„ë£Œ ===\")\n",
    "print(f\"ì´ ë¹„ì–´ìˆë˜ ë¬¸ì„œ: {total_count}ê±´\")\n",
    "print(f\"ìˆ˜ì • ì„±ê³µ: {fixed_count}ê±´\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
