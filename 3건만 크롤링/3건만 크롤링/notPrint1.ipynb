{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503d343f-ada4-49cf-9d37-608fa6afeb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\leejuhwan\\miniconda3\\lib\\site-packages (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe47225-dd02-4db7-b503-a09f226cf1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“‹ ë¬¸ì„œID ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘\n",
      "   ëŒ€ìƒ ë¬¸ì„œ: 3ê±´\n",
      "   ë¬¸ì„œID: ['2002390', '2008214', '2008497']\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ ì „ìê²°ì¬ ë©”ë‰´ ì§„ì… ì¤‘...\n",
      "\n",
      "======================================================================\n",
      "[1/3] ë¬¸ì„œID: 2002390 ì²˜ë¦¬ ì¤‘...\n",
      "======================================================================\n",
      "  ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°...\n",
      "\n",
      "  ğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n",
      "  ğŸ“¥ [1/1] ë‹¤ìš´ë¡œë“œ ì¤‘: ì¹´ë“œì‚¬ìš©ì‹¤ì ë³´ê³ (20100527)_í¸ì˜ìˆ˜K.xls (96.50KB)\n",
      "      âœ… ì €ì¥: 1764220907894_ì¹´ë“œì‚¬ìš©ì‹¤ì ë³´ê³ (20100527)_í¸ì˜ìˆ˜K.xls (96.50KB) (98,816 bytes)\n",
      "\n",
      "  ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\n",
      "\n",
      "  âœ… JSON ì €ì¥: detailed_docs\\doc_2002390.json\n",
      "  âœ… ìˆ˜ì§‘ ì™„ë£Œ: ë²•ì¸ì¹´ë“œÂ ì‚¬ìš©ë‚´ì—­Â ì •ì‚°Â ë³´ê³ _5ì›”_í¸ì˜ìˆ˜K\n",
      "\n",
      "======================================================================\n",
      "[2/3] ë¬¸ì„œID: 2008214 ì²˜ë¦¬ ì¤‘...\n",
      "======================================================================\n",
      "  ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°...\n",
      "\n",
      "  ğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n",
      "  ğŸ“¥ [1/1] ë‹¤ìš´ë¡œë“œ ì¤‘: ë²•ì¸ì¹´ë“œ_ì‚¬ìš©ë‚´ì—­ë³´ê³ _ì†¡ì¤€ì„ (201501).xlsx (91.81KB)\n",
      "      âœ… ì €ì¥: 1764220915363_ë²•ì¸ì¹´ë“œ_ì‚¬ìš©ë‚´ì—­ë³´ê³ _ì†¡ì¤€ì„ (201501).xlsx (91.81KB) (94,015 bytes)\n",
      "\n",
      "  ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\n",
      "\n",
      "  âœ… JSON ì €ì¥: detailed_docs\\doc_2008214.json\n",
      "  âœ… ìˆ˜ì§‘ ì™„ë£Œ: 2015ë…„Â 1ì›”Â CARDÂ ì‚¬ìš©ë‚´ì—­Â ì •ì‚°Â ë³´ê³ \t2015ë…„01ì›”Â ë²•ì¸ì¹´ë“œÂ ì •ì‚°ë‚´ì—­Â í’ˆì˜(ì†¡ì¤€ì„ \n",
      "\n",
      "======================================================================\n",
      "[3/3] ë¬¸ì„œID: 2008497 ì²˜ë¦¬ ì¤‘...\n",
      "======================================================================\n",
      "  ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°...\n",
      "\n",
      "  ğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n",
      "  ğŸ“¥ [1/3] ë‹¤ìš´ë¡œë“œ ì¤‘: 20150330_ë³´ì•ˆì„œì•½ì„œ_ë°•ì„ì€.doc (34.00KB)\n",
      "      âœ… ì €ì¥: 1764220922670_20150330_ë³´ì•ˆì„œì•½ì„œ_ë°•ì„ì€.doc (34.00KB) (34,816 bytes)\n",
      "  ğŸ“¥ [2/3] ë‹¤ìš´ë¡œë“œ ì¤‘: 20150330_í† í”¼ì•„ì •ë³´ê¸°ìˆ _LGD_ê°œë°œìš©ì—­ê³„ì•½ì„œ_ë°•ì„ì€.doc (72.00KB)\n",
      "      âœ… ì €ì¥: 1764220922706_20150330_í† í”¼ì•„ì •ë³´ê¸°ìˆ _LGD_ê°œë°œìš©ì—­ê³„ì•½ì„œ_ë°•ì„ì€.doc (72.00KB) (73,728 bytes)\n",
      "  ğŸ“¥ [3/3] ë‹¤ìš´ë¡œë“œ ì¤‘: LGD SWP Project On-Time Visibility í™•ë³´_ì‹¤ì  í™”ë©´ êµ¬í˜„.jpg (132.85KB)\n",
      "      âœ… ì €ì¥: 1764220922734_LGD SWP Project On-Time Visibility í™•ë³´_ì‹¤ì  í™”ë©´ êµ¬í˜„.jpg (132.85KB) (136,041 bytes)\n",
      "\n",
      "  ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\n",
      "\n",
      "  âœ… JSON ì €ì¥: detailed_docs\\doc_2008497.json\n",
      "  âœ… ìˆ˜ì§‘ ì™„ë£Œ: [ì™¸ì£¼ì¸ë ¥ê³„ì•½í’ˆì˜-ëŒ€ì™¸]LGDÂ SWPÂ ProjectÂ On-TimeÂ VisibilityÂ í™•ë³´\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ!\n",
      "======================================================================\n",
      "  âœ… ì„±ê³µ: 3ê±´\n",
      "  âŒ ì‹¤íŒ¨: 0ê±´\n",
      "\n",
      "ğŸ“ ì €ì¥ ì™„ë£Œ:\n",
      "   - ì „ìê²°ì¬_ëª©ë¡.xlsx\n",
      "   - ì „ìê²°ì¬_ëª©ë¡.csv\n",
      "   - ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.xlsx (5ê±´)\n",
      "   - ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.csv\n",
      "\n",
      "ğŸ“‚ ìƒì„¸ JSON: detailed_docs/ í´ë”\n",
      "ğŸ“‚ ì²¨ë¶€íŒŒì¼: detailed_docs/attachments/[ë¬¸ì„œID]/\n",
      "ğŸ“‚ ë³¸ë¬¸ì´ë¯¸ì§€: detailed_docs/images/[ë¬¸ì„œID]/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œIDë¡œ ì§ì ‘ ìƒì„¸ í¬ë¡¤ë§ + ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "# DB ì €ì¥ ê¸°ëŠ¥ ì œì™¸ ë²„ì „\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "import time, re\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE = \"http://office.anyfive.com/home/\"\n",
    "\n",
    "# â­ í¬ë¡¤ë§í•  ë¬¸ì„œID ëª©ë¡ (ì—¬ê¸°ì— ì›í•˜ëŠ” ë¬¸ì„œID ì…ë ¥)\n",
    "TARGET_DOC_IDS = [\n",
    "    \"2002390\",\n",
    "    \"2008214\", \n",
    "    \"2008497\",\n",
    "]\n",
    "\n",
    "# BeautifulSoup parser\n",
    "try:\n",
    "    import lxml\n",
    "    BS_PARSER = \"lxml\"\n",
    "except:\n",
    "    BS_PARSER = \"html.parser\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê³µí†µ ìœ í‹¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "driver = None\n",
    "\n",
    "def wait_click_css(css, sec=20):\n",
    "    return WebDriverWait(driver, sec).until(EC.element_to_be_clickable((By.CSS_SELECTOR, css)))\n",
    "\n",
    "def wait_css(css, sec=20):\n",
    "    return WebDriverWait(driver, sec).until(EC.presence_of_element_located((By.CSS_SELECTOR, css)))\n",
    "\n",
    "def click_in_all_frames(css_list=None, xp_list=None, text_contains=None):\n",
    "    \"\"\"ì—¬ëŸ¬ í”„ë ˆì„ì„ í›‘ìœ¼ë©´ì„œ í´ë¦­ ì‹œë„\"\"\"\n",
    "    driver.switch_to.default_content()\n",
    "    if css_list:\n",
    "        for css in css_list:\n",
    "            els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "            if els:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", els[0])\n",
    "                driver.execute_script(\"arguments[0].click();\", els[0])\n",
    "                return True\n",
    "    if xp_list:\n",
    "        for xp in xp_list:\n",
    "            els = driver.find_elements(By.XPATH, xp)\n",
    "            if els:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", els[0])\n",
    "                driver.execute_script(\"arguments[0].click();\", els[0])\n",
    "                return True\n",
    "    if text_contains:\n",
    "        els = driver.find_elements(By.XPATH, \"//*[self::a or self::button or self::li or self::span]\")\n",
    "        for el in els:\n",
    "            if text_contains in (el.text or \"\"):\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                driver.execute_script(\"arguments[0].click();\", el)\n",
    "                return True\n",
    "    frames = driver.find_elements(By.CSS_SELECTOR, \"iframe, frame\")\n",
    "    for fr in frames:\n",
    "        driver.switch_to.default_content()\n",
    "        driver.switch_to.frame(fr)\n",
    "        if css_list:\n",
    "            for css in css_list:\n",
    "                els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "                if els:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", els[0])\n",
    "                    driver.execute_script(\"arguments[0].click();\", els[0])\n",
    "                    return True\n",
    "        if xp_list:\n",
    "            for xp in xp_list:\n",
    "                els = driver.find_elements(By.XPATH, xp)\n",
    "                if els:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", els[0])\n",
    "                    driver.execute_script(\"arguments[0].click();\", els[0])\n",
    "                    return True\n",
    "        if text_contains:\n",
    "            els = driver.find_elements(By.XPATH, \"//*[self::a or self::button or self::li or self::span]\")\n",
    "            for el in els:\n",
    "                if text_contains in (el.text or \"\"):\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    driver.execute_script(\"arguments[0].click();\", el)\n",
    "                    return True\n",
    "    driver.switch_to.default_content()\n",
    "    return False\n",
    "\n",
    "def switch_into_frame_having(selector_css):\n",
    "    \"\"\"selector_css ìš”ì†Œê°€ ë³´ì´ëŠ” í”„ë ˆì„ìœ¼ë¡œ ì „í™˜\"\"\"\n",
    "    driver.switch_to.default_content()\n",
    "    if driver.find_elements(By.CSS_SELECTOR, selector_css):\n",
    "        return True\n",
    "    for fr in driver.find_elements(By.CSS_SELECTOR, \"iframe, frame\"):\n",
    "        driver.switch_to.default_content()\n",
    "        driver.switch_to.frame(fr)\n",
    "        if driver.find_elements(By.CSS_SELECTOR, selector_css):\n",
    "            return True\n",
    "    driver.switch_to.default_content()\n",
    "    return False\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    s = re.sub(r'[\\t\\r\\n]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "def dedupe_attachments(items):\n",
    "    seen, out = set(), []\n",
    "    for it in items or []:\n",
    "        name = (it.get(\"íŒŒì¼ëª…\") or \"\").strip()\n",
    "        href = (it.get(\"href\") or \"\").strip()\n",
    "        key = (name, href)\n",
    "        if not name and not href:\n",
    "            continue\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        clean = {k: v for k, v in it.items() if v not in (None, \"\", [])}\n",
    "        out.append(clean)\n",
    "    return out\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# íŒŒì¼ í™•ì¥ì ê°ì§€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_file_extension(url, content_type=None, content_bytes=None, default_ext=\".bin\"):\n",
    "    \"\"\"URLê³¼ Content-Typeì—ì„œ ì˜¬ë°”ë¥¸ í™•ì¥ì ì¶”ì¶œ\"\"\"\n",
    "    if content_bytes:\n",
    "        if content_bytes[:2] == b'\\xff\\xd8':\n",
    "            return '.jpg'\n",
    "        elif content_bytes[:8] == b'\\x89PNG\\r\\n\\x1a\\n':\n",
    "            return '.png'\n",
    "        elif content_bytes[:6] in (b'GIF87a', b'GIF89a'):\n",
    "            return '.gif'\n",
    "        elif content_bytes[:4] == b'RIFF' and content_bytes[8:12] == b'WEBP':\n",
    "            return '.webp'\n",
    "        elif content_bytes[:2] == b'BM':\n",
    "            return '.bmp'\n",
    "    \n",
    "    if content_type:\n",
    "        ct = content_type.split(';')[0].strip().lower()\n",
    "        if 'jpeg' in ct or 'jpg' in ct:\n",
    "            return '.jpg'\n",
    "        if 'png' in ct:\n",
    "            return '.png'\n",
    "        if 'gif' in ct:\n",
    "            return '.gif'\n",
    "        if 'webp' in ct:\n",
    "            return '.webp'\n",
    "        ext = mimetypes.guess_extension(ct)\n",
    "        if ext:\n",
    "            return '.jpg' if ext == '.jpe' else ext\n",
    "    \n",
    "    path = urlparse(url).path\n",
    "    if '.' in path.split('/')[-1]:\n",
    "        ext = Path(path).suffix.lower()\n",
    "        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg',\n",
    "                   '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.txt']:\n",
    "            return ext\n",
    "    \n",
    "    return default_ext\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"íŒŒì¼ëª…ì—ì„œ ìœ„í—˜í•œ ë¬¸ì ì œê±°\"\"\"\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "    if len(filename) > 200:\n",
    "        name, ext = Path(filename).stem, Path(filename).suffix\n",
    "        filename = name[:190] + ext\n",
    "    return filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ìƒì„¸ í˜ì´ì§€ íŒŒì‹± í•¨ìˆ˜ë“¤\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_session_from_driver(drv, base_url):\n",
    "    \"\"\"Selenium ì¿ í‚¤ë¥¼ requests Sessionìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    s = requests.Session()\n",
    "    host = urlparse(base_url).hostname\n",
    "    for c in drv.get_cookies():\n",
    "        s.cookies.set(c[\"name\"], c[\"value\"], domain=c.get(\"domain\") or host)\n",
    "    s.headers.update({\"X-Requested-With\": \"XMLHttpRequest\"})\n",
    "    return s\n",
    "\n",
    "def parse_kv_tables(drv):\n",
    "    \"\"\"ìƒì„¸ í˜ì´ì§€ ìƒë‹¨ Key-Value í…Œì´ë¸” íŒŒì‹±\"\"\"\n",
    "    soup = BeautifulSoup(drv.page_source, BS_PARSER)\n",
    "    out = {}\n",
    "    \n",
    "    title = soup.select_one(\".apr_title\")\n",
    "    if title:\n",
    "        out[\"ì œëª©\"] = title.get_text(\" \", strip=True)\n",
    "    \n",
    "    for tbl in soup.select(\"table.tbl_none-titInfo\"):\n",
    "        for tr in tbl.select(\"tr\"):\n",
    "            ths = [th.get_text(\" \", strip=True) for th in tr.select(\"th\")]\n",
    "            tds = [td.get_text(\"\\n\", strip=True) for td in tr.select(\"td\")]\n",
    "            for i in range(min(len(ths), len(tds))):\n",
    "                key = ths[i].replace(\" \", \"\")\n",
    "                val = re.sub(r\"\\n+\", \"\\n\", tds[i]).strip()\n",
    "                if \"ê¸°ì•ˆì¼ì™„ë£Œì¼\" in key or (\"ê¸°ì•ˆì¼\" in key and \"ì™„ë£Œ\" in key):\n",
    "                    dts = re.findall(r\"\\d{4}[-/]\\d{2}[-/]\\d{2}(?:\\s+\\d{2}:\\d{2}:\\d{2})?\", val)\n",
    "                    if len(dts) >= 1:\n",
    "                        out[\"ê¸°ì•ˆì¼\"] = dts[0].replace(\"-\", \"/\")\n",
    "                    if len(dts) >= 2:\n",
    "                        out[\"ì™„ë£Œì¼\"] = dts[1].replace(\"-\", \"/\")\n",
    "                else:\n",
    "                    out[key] = val\n",
    "    \n",
    "    attach = []\n",
    "    for a in soup.select(\"#aprShowFileMap a\"):\n",
    "        attach.append({\n",
    "            \"íŒŒì¼ëª…\": a.get_text(\" \", strip=True),\n",
    "            \"href\": urljoin(BASE, a.get(\"href\", \"\")),\n",
    "        })\n",
    "    if attach:\n",
    "        out[\"_ì²¨ë¶€íŒŒì¼_anchor\"] = attach\n",
    "    \n",
    "    return out\n",
    "\n",
    "def download_attachments(attachments, session, base_url, doc_id, out_dir=\"detailed_docs/attachments\"):\n",
    "    \"\"\"ì²¨ë¶€íŒŒì¼ ì‹¤ì œ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
    "    if not attachments:\n",
    "        print(f\"  â„¹ï¸  ì²¨ë¶€íŒŒì¼ ì—†ìŒ\")\n",
    "        return []\n",
    "    \n",
    "    att_dir = Path(out_dir) / str(doc_id)\n",
    "    att_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for i, att in enumerate(attachments, 1):\n",
    "        href = att.get(\"href\", \"\")\n",
    "        if not href:\n",
    "            continue\n",
    "        \n",
    "        filename = att.get(\"íŒŒì¼ëª…\", f\"attachment_{i}\")\n",
    "        abs_url = urljoin(base_url, href)\n",
    "        \n",
    "        try:\n",
    "            print(f\"  ğŸ“¥ [{i}/{len(attachments)}] ë‹¤ìš´ë¡œë“œ ì¤‘: {filename}\")\n",
    "            r = session.get(abs_url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            content_type = r.headers.get('Content-Type', '')\n",
    "            ext = get_file_extension(abs_url, content_type, content_bytes=r.content)\n",
    "            \n",
    "            safe_name = sanitize_filename(filename)\n",
    "            if not Path(safe_name).suffix:\n",
    "                safe_name = safe_name + ext\n",
    "            \n",
    "            final_name = f\"{int(time.time()*1000)}_{safe_name}\"\n",
    "            file_path = att_dir / final_name\n",
    "            file_path.write_bytes(r.content)\n",
    "            \n",
    "            results.append({\n",
    "                **att,\n",
    "                \"saved_as\": str(file_path),\n",
    "                \"saved_filename\": final_name,\n",
    "                \"size\": len(r.content),\n",
    "                \"content_type\": content_type\n",
    "            })\n",
    "            success_count += 1\n",
    "            print(f\"      âœ… ì €ì¥: {file_path.name} ({len(r.content):,} bytes)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({**att, \"error\": str(e)})\n",
    "            print(f\"      âŒ ì‹¤íŒ¨: {filename} - {e}\")\n",
    "    \n",
    "    if results:\n",
    "        manifest = {\n",
    "            \"ë¬¸ì„œID\": doc_id,\n",
    "            \"ì´íŒŒì¼ìˆ˜\": len(attachments),\n",
    "            \"ì„±ê³µ\": success_count,\n",
    "            \"ì‹¤íŒ¨\": len(attachments) - success_count,\n",
    "            \"íŒŒì¼ëª©ë¡\": results\n",
    "        }\n",
    "        manifest_path = att_dir / \"_manifest.json\"\n",
    "        manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_body_bundle(drv, base_url, doc_id, out_dir, session):\n",
    "    \"\"\"ë³¸ë¬¸ HTML/í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€ ì¶”ì¶œ\"\"\"\n",
    "    body_html = drv.execute_script(\"\"\"\n",
    "      const pick = () => {\n",
    "        const sel = ['#appr_doc_cont', '#apprDocContent', '#tempApprDocContent', '.content_box'];\n",
    "        for (const s of sel) {\n",
    "          const el = document.querySelector(s);\n",
    "          if (el) return el.innerHTML;\n",
    "        }\n",
    "        return '';\n",
    "      };\n",
    "      return pick();\n",
    "    \"\"\") or \"\"\n",
    "    \n",
    "    if not body_html.strip():\n",
    "        for f in drv.find_elements(By.CSS_SELECTOR, \"iframe[id^='editorIframe_approvalShow_']\"):\n",
    "            drv.switch_to.frame(f)\n",
    "            body_html = drv.execute_script(\"return document.body ? document.body.innerHTML : '';\") or \"\"\n",
    "            drv.switch_to.default_content()\n",
    "            break\n",
    "    \n",
    "    bundle = {\"body_html\": body_html, \"body_text\": \"\", \"body_tables\": [], \"body_images\": []}\n",
    "    \n",
    "    if body_html.strip():\n",
    "        soup = BeautifulSoup(body_html, BS_PARSER)\n",
    "        for t in soup([\"script\", \"style\"]):\n",
    "            t.decompose()\n",
    "        txt = soup.get_text(\"\\n\")\n",
    "        bundle[\"body_text\"] = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "        \n",
    "        try:\n",
    "            from io import StringIO\n",
    "            dfs = pd.read_html(StringIO(body_html))\n",
    "            for df in dfs:\n",
    "                df.columns = [str(c).strip() for c in df.columns]\n",
    "                bundle[\"body_tables\"].append(df.fillna(\"\").to_dict(orient=\"records\"))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        images = soup.select(\"img[src]\")\n",
    "        if images:\n",
    "            img_dir = Path(out_dir) / \"images\" / str(doc_id)\n",
    "            img_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for i, img in enumerate(images, 1):\n",
    "                src = img.get(\"src\", \"\").strip()\n",
    "                if not src:\n",
    "                    continue\n",
    "                abs_url = urljoin(base_url, src)\n",
    "                \n",
    "                try:\n",
    "                    r = session.get(abs_url, timeout=15)\n",
    "                    r.raise_for_status()\n",
    "                    \n",
    "                    content_type = r.headers.get('Content-Type', '')\n",
    "                    ext = get_file_extension(abs_url, content_type, content_bytes=r.content, default_ext=\".png\")\n",
    "                    \n",
    "                    fn = f\"img_{int(time.time()*1000)}_{i}{ext}\"\n",
    "                    p = img_dir / fn\n",
    "                    p.write_bytes(r.content)\n",
    "                    \n",
    "                    bundle[\"body_images\"].append({\n",
    "                        \"src\": abs_url,\n",
    "                        \"saved_as\": str(p),\n",
    "                        \"size\": len(r.content)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    bundle[\"body_images\"].append({\"src\": abs_url, \"error\": str(e)})\n",
    "    \n",
    "    return bundle\n",
    "\n",
    "def parse_approval_line_table(drv):\n",
    "    \"\"\"ê²°ì¬ì„  í…Œì´ë¸” íŒŒì‹±\"\"\"\n",
    "    soup = BeautifulSoup(drv.page_source, BS_PARSER)\n",
    "    out = []\n",
    "    for tr in soup.select(\"#apprLineTable tbody tr\"):\n",
    "        tds = [td.get_text(\" \", strip=True) for td in tr.select(\"td\")]\n",
    "        if len(tds) >= 6:\n",
    "            out.append({\n",
    "                \"ìˆœì„œ\": tds[0], \"êµ¬ë¶„\": tds[1], \"ìƒíƒœ\": tds[2],\n",
    "                \"ê²°ì¬ì¼ì‹œ\": tds[3], \"ê²°ì¬ë¶€ì„œ\": tds[4], \"ê²°ì¬ì\": tds[5]\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def parse_approval_opinions(drv):\n",
    "    \"\"\"ê²°ì¬ì˜ê²¬ íŒŒì‹±\"\"\"\n",
    "    soup = BeautifulSoup(drv.page_source, BS_PARSER)\n",
    "    results = []\n",
    "    \n",
    "    ul_list = soup.select('table.tbl_none-titInfo ul._mp, table.tbl_none-titInfo ul._np')\n",
    "    for ul in ul_list:\n",
    "        items = ul.find_all('li')\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        head = items[0]\n",
    "        body = items[1]\n",
    "        \n",
    "        code_el = head.select_one('[class^=approvalOpinionCode]') or head.find('span')\n",
    "        code = code_el.get_text(strip=True) if code_el else ''\n",
    "        \n",
    "        user_a = head.select_one('span.F_12_black_b a.abcUsr')\n",
    "        user = user_a.get_text(strip=True) if user_a else ''\n",
    "        \n",
    "        dt = ''\n",
    "        for sp in head.find_all('span'):\n",
    "            t = sp.get_text(strip=True)\n",
    "            if t and t[0].isdigit() and ('-' in t or '/' in t) and ':' in t:\n",
    "                dt = t\n",
    "        \n",
    "        msg = body.get_text(\" \", strip=True)\n",
    "        results.append({\n",
    "            \"ë‹¨ê³„\": code,\n",
    "            \"ì‘ì„±ì\": user,\n",
    "            \"ì‹œê°„\": dt,\n",
    "            \"ë‚´ìš©\": msg\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_comments(session, base_url, apr_doc_seq):\n",
    "    \"\"\"ëŒ“ê¸€ XHR\"\"\"\n",
    "    try:\n",
    "        url = urljoin(base_url, \"/apr/doc/aprCmtlist.jstl\")\n",
    "        r = session.get(url, params={\"aprDocSeq\": apr_doc_seq}, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, BS_PARSER)\n",
    "        out = []\n",
    "        for ul in soup.select(\"ul._mp\"):\n",
    "            head = ul.select_one(\"li > span.F_12_black_b\")\n",
    "            code = ul.select_one(\"li > span[class^=approvalOpinionCode]\")\n",
    "            time_el = ul.select_one(\"li > span[style*='margin-left']\")\n",
    "            body = ul.select_one(\"li + li\")\n",
    "            out.append({\n",
    "                \"ë‹¨ê³„\": code.get_text(strip=True) if code else \"\",\n",
    "                \"ì‘ì„±ì\": head.get_text(\" \", strip=True) if head else \"\",\n",
    "                \"ì‹œê°„\": time_el.get_text(strip=True) if time_el else \"\",\n",
    "                \"ë‚´ìš©\": body.get_text(\"\\n\", strip=True) if body else \"\"\n",
    "            })\n",
    "        return out\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def scrape_detailed_doc(drv, doc_id, base_url=BASE, out_dir=\"detailed_docs\"):\n",
    "    \"\"\"ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "    WebDriverWait(drv, 15).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(drv, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".apr_title\")))\n",
    "    except:\n",
    "        print(f\"  âš ï¸ ì œëª© ìš”ì†Œ ëŒ€ê¸° ì‹¤íŒ¨, ê³„ì† ì§„í–‰...\")\n",
    "    \n",
    "    apr_doc_seq = drv.execute_script(\"var el=document.querySelector('input#appr_doc_seq');return el?el.value:'';\")\n",
    "    if not apr_doc_seq:\n",
    "        apr_doc_seq = doc_id\n",
    "    \n",
    "    session = make_session_from_driver(drv, base_url)\n",
    "    \n",
    "    # ìƒë‹¨ ì •ë³´\n",
    "    kv = parse_kv_tables(drv)\n",
    "    \n",
    "    # ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘ ë° ë‹¤ìš´ë¡œë“œ\n",
    "    print(f\"\\n  ğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
    "    attach_from_dom = []\n",
    "    soup = BeautifulSoup(drv.page_source, BS_PARSER)\n",
    "    for a in soup.select(\"#aprShowFileMap a\"):\n",
    "        attach_from_dom.append({\n",
    "            \"íŒŒì¼ëª…\": a.get_text(\" \", strip=True),\n",
    "            \"href\": urljoin(base_url, a.get(\"href\", \"\")),\n",
    "        })\n",
    "    \n",
    "    anchor_from_kv = kv.get(\"_ì²¨ë¶€íŒŒì¼_anchor\") or []\n",
    "    attach_list = dedupe_attachments(anchor_from_kv + attach_from_dom)\n",
    "    attach = download_attachments(attach_list, session, base_url, apr_doc_seq, out_dir + \"/attachments\")\n",
    "    \n",
    "    # ë³¸ë¬¸\n",
    "    print(f\"\\n  ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...\")\n",
    "    body_bundle = extract_body_bundle(drv, base_url, apr_doc_seq, out_dir, session)\n",
    "    \n",
    "    # ëŒ“ê¸€\n",
    "    comments = fetch_comments(session, base_url, apr_doc_seq)\n",
    "    \n",
    "    # ê²°ì¬ì„ \n",
    "    appr_grid = parse_approval_line_table(drv)\n",
    "    \n",
    "    # ê²°ì¬ì˜ê²¬\n",
    "    approval_opinions = parse_approval_opinions(drv)\n",
    "    \n",
    "    # ìµœì¢… ë¬¸ì„œ êµ¬ì„±\n",
    "    doc = {\n",
    "        \"ë¬¸ì„œID\": doc_id,\n",
    "        \"appr_doc_seq\": apr_doc_seq,\n",
    "        \"ìƒë‹¨ì •ë³´\": kv,\n",
    "        \"ì²¨ë¶€íŒŒì¼\": attach,\n",
    "        \"ê²°ì¬ì„ (í‘œ)\": appr_grid,\n",
    "        \"ë³¸ë¬¸\": body_bundle,\n",
    "        \"ëŒ“ê¸€\": comments,\n",
    "        \"ê²°ì¬ì˜ê²¬\": approval_opinions,\n",
    "    }\n",
    "    \n",
    "    # JSON ì €ì¥\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    json_path = out_path / f\"doc_{doc_id}.json\"\n",
    "    json_path.write_text(json.dumps(doc, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"\\n  âœ… JSON ì €ì¥: {json_path}\")\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def normalize_for_export(doc):\n",
    "    \"\"\"DataFrameìš© ì •ê·œí™”\"\"\"\n",
    "    top = doc.get(\"ìƒë‹¨ì •ë³´\", {}) or {}\n",
    "    body = doc.get(\"ë³¸ë¬¸\", {}) or {}\n",
    "    \n",
    "    attach = dedupe_attachments((top.get(\"_ì²¨ë¶€íŒŒì¼_anchor\") or []) + (doc.get(\"ì²¨ë¶€íŒŒì¼\") or []))\n",
    "    \n",
    "    content = {\n",
    "        \"ìƒë‹¨ì •ë³´\": top,\n",
    "        \"ë³¸ë¬¸\": body,\n",
    "        \"ì²¨ë¶€íŒŒì¼\": attach,\n",
    "        \"ê²°ì¬ì„ _í‘œ\": doc.get(\"ê²°ì¬ì„ (í‘œ)\") or [],\n",
    "        \"ê²°ì¬ì˜ê²¬\": doc.get(\"ê²°ì¬ì˜ê²¬\") or [],\n",
    "        \"ëŒ“ê¸€\": doc.get(\"ëŒ“ê¸€\") or [],\n",
    "    }\n",
    "    \n",
    "    row = {\n",
    "        \"ë¬¸ì„œID\": doc.get(\"ë¬¸ì„œID\") or doc.get(\"appr_doc_seq\") or \"\",\n",
    "        \"ì œëª©\": top.get(\"ì œëª©\") or \"\",\n",
    "        \"ì–‘ì‹ëª…\": top.get(\"ì–‘ì‹ëª…\", \"\"),\n",
    "        \"ë¬¸ì„œë²ˆí˜¸\": top.get(\"ë¬¸ì„œë²ˆí˜¸\", \"\"),\n",
    "        \"ê¸°ì•ˆì\": top.get(\"ê¸°ì•ˆì\", \"\"),\n",
    "        \"ê¸°ì•ˆì¼\": top.get(\"ê¸°ì•ˆì¼\", \"\"),\n",
    "        \"ìƒíƒœ\": top.get(\"ìƒíƒœ\", \"\"),\n",
    "        \"ë‚´ìš©\": json.dumps(content, ensure_ascii=False)\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def back_to_list():\n",
    "    \"\"\"ìƒì„¸ì—ì„œ ëª©ë¡ìœ¼ë¡œ ë³µê·€\"\"\"\n",
    "    try:\n",
    "        driver.switch_to.default_content()\n",
    "        switch_into_frame_having(\".tabConts_headerBx, #apprDocContent\")\n",
    "        \n",
    "        selectors = [\n",
    "            \"button.k-button[onclick*='onClick_findFormBtn']\",\n",
    "            \"button[onclick*='onClick_findFormBtn']\",\n",
    "            \"button[title='ëª©ë¡']\",\n",
    "        ]\n",
    "        for css in selectors:\n",
    "            els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "            if els:\n",
    "                driver.execute_script(\"arguments[0].click();\", els[0])\n",
    "                time.sleep(1.5)\n",
    "                return True\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(1.5)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    global driver\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ“‹ ë¬¸ì„œID ê¸°ë°˜ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘\")\n",
    "    print(f\"   ëŒ€ìƒ ë¬¸ì„œ: {len(TARGET_DOC_IDS)}ê±´\")\n",
    "    print(f\"   ë¬¸ì„œID: {TARGET_DOC_IDS}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ë¸Œë¼ìš°ì € ì„¤ì •\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-popup-blocking\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option('useAutomationExtension', False)\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(), options=opts)\n",
    "    driver.get(BASE)\n",
    "    time.sleep(1.2)\n",
    "    \n",
    "    input(\"\\nğŸ” ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”... \")\n",
    "    \n",
    "    # ì „ìê²°ì¬ â†’ ê²°ì¬ ë¬¸ì„œê´€ë¦¬ ë©”ë‰´ ì§„ì…\n",
    "    print(\"\\nğŸ“ ì „ìê²°ì¬ ë©”ë‰´ ì§„ì… ì¤‘...\")\n",
    "    click_in_all_frames(\n",
    "        css_list=[\"a[href='/apr/'].left_menu\", \"a[href='/apr/']\"],\n",
    "        xp_list=[\"//a[@href='/apr/']\"],\n",
    "        text_contains=\"ì „ìê²°ì¬\"\n",
    "    )\n",
    "    time.sleep(0.8)\n",
    "    \n",
    "    click_in_all_frames(\n",
    "        css_list=[\"li[onclick*='managementDoc']\"],\n",
    "        xp_list=[\"//*[@onclick and contains(.,'managementDoc')]\"],\n",
    "        text_contains=\"ê²°ì¬ ë¬¸ì„œê´€ë¦¬\"\n",
    "    )\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    # í”„ë ˆì„ ì „í™˜\n",
    "    switch_into_frame_having(\"header, .tabConts_headerBx\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ìš©\n",
    "    details = []\n",
    "    attachments_rows = []\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # ê° ë¬¸ì„œIDì— ëŒ€í•´ í¬ë¡¤ë§\n",
    "    for idx, doc_id in enumerate(TARGET_DOC_IDS):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{idx+1}/{len(TARGET_DOC_IDS)}] ë¬¸ì„œID: {doc_id} ì²˜ë¦¬ ì¤‘...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # í”„ë ˆì„ í™•ì¸\n",
    "            driver.switch_to.default_content()\n",
    "            switch_into_frame_having(\"header, .tabConts_headerBx, #apprDocContent\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # â­ ë¬¸ì„œIDë¡œ ì§ì ‘ ìƒì„¸ ì—´ê¸° (í•µì‹¬!)\n",
    "            print(f\"  ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°...\")\n",
    "            driver.execute_script(f\"managementDocList.clickGridRow({doc_id});\")\n",
    "            time.sleep(2.5)\n",
    "            \n",
    "            # ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°\n",
    "            try:\n",
    "                WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#apprLineTable, .apr_title\"))\n",
    "                )\n",
    "            except:\n",
    "                print(f\"  âš ï¸ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹¤íŒ¨\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "            # ìƒì„¸ ìˆ˜ì§‘\n",
    "            doc_json = scrape_detailed_doc(driver, doc_id, base_url=BASE, out_dir=\"detailed_docs\")\n",
    "            \n",
    "            # ì •ê·œí™”\n",
    "            flat = normalize_for_export(doc_json)\n",
    "            details.append(flat)\n",
    "            \n",
    "            # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ\n",
    "            for att in doc_json.get(\"ì²¨ë¶€íŒŒì¼\", []):\n",
    "                att_row = {\n",
    "                    \"ë¬¸ì„œID\": doc_id,\n",
    "                    \"íŒŒì¼ì´ë¦„\": att.get(\"íŒŒì¼ëª…\", \"\"),\n",
    "                    \"ê²½ë¡œ\": att.get(\"href\", \"\"),\n",
    "                    \"ì €ì¥ê²½ë¡œ\": att.get(\"saved_as\", \"\"),\n",
    "                    \"íŒŒì¼í¬ê¸°\": att.get(\"size\", 0),\n",
    "                    \"content_type\": att.get(\"content_type\", \"\")\n",
    "                }\n",
    "                attachments_rows.append(att_row)\n",
    "            \n",
    "            success_count += 1\n",
    "            print(f\"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {flat.get('ì œëª©', '')[:50]}\")\n",
    "            \n",
    "            # ëª©ë¡ìœ¼ë¡œ ë³µê·€\n",
    "            back_to_list()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"  âŒ ì˜¤ë¥˜: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            try:\n",
    "                back_to_list()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  âœ… ì„±ê³µ: {success_count}ê±´\")\n",
    "    print(f\"  âŒ ì‹¤íŒ¨: {error_count}ê±´\")\n",
    "    \n",
    "    if details:\n",
    "        # Excel/CSV ì €ì¥\n",
    "        cols = [\"ë¬¸ì„œID\", \"ì œëª©\", \"ì–‘ì‹ëª…\", \"ë¬¸ì„œë²ˆí˜¸\", \"ê¸°ì•ˆì\", \"ê¸°ì•ˆì¼\", \"ìƒíƒœ\", \"ë‚´ìš©\"]\n",
    "        df = pd.DataFrame(details, columns=cols)\n",
    "        \n",
    "        for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "            df[c] = df[c].map(lambda x: norm_space(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        df.to_excel(\"ì „ìê²°ì¬_ëª©ë¡.xlsx\", index=False, engine=\"openpyxl\")\n",
    "        df.to_csv(\"ì „ìê²°ì¬_ëª©ë¡.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\nğŸ“ ì €ì¥ ì™„ë£Œ:\")\n",
    "        print(f\"   - ì „ìê²°ì¬_ëª©ë¡.xlsx\")\n",
    "        print(f\"   - ì „ìê²°ì¬_ëª©ë¡.csv\")\n",
    "        \n",
    "        # ì²¨ë¶€íŒŒì¼ ëª©ë¡ ì €ì¥\n",
    "        if attachments_rows:\n",
    "            att_df = pd.DataFrame(attachments_rows)\n",
    "            att_df.to_excel(\"ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.xlsx\", index=False, engine=\"openpyxl\")\n",
    "            att_df.to_csv(\"ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"   - ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.xlsx ({len(attachments_rows)}ê±´)\")\n",
    "            print(f\"   - ì „ìê²°ì¬_ì²¨ë¶€íŒŒì¼.csv\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‚ ìƒì„¸ JSON: detailed_docs/ í´ë”\")\n",
    "    print(f\"ğŸ“‚ ì²¨ë¶€íŒŒì¼: detailed_docs/attachments/[ë¬¸ì„œID]/\")\n",
    "    print(f\"ğŸ“‚ ë³¸ë¬¸ì´ë¯¸ì§€: detailed_docs/images/[ë¬¸ì„œID]/\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
